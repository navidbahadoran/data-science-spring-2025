{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7233730b",
   "metadata": {},
   "source": [
    "# Boosting\n",
    "\n",
    "## What we will accomplish\n",
    "\n",
    "In this notebook we will:\n",
    "- Introduce the concepts of:\n",
    "    - Weak learners and \n",
    "    - Strong learners and\n",
    "- Give a description of boosting methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e2cfb7",
   "metadata": {},
   "source": [
    "## Boosting\n",
    "\n",
    "Last week we learned that a bagging ensemble of a model which over-fits can help to correct over-fitting.\n",
    "\n",
    "This week we will learn about **boosting** which address under-fitting.  While this is an oversimplification, the basic idea is that a boosted model fits sequence of models, each one trained on the residuals of the last.\n",
    "\n",
    "The theory behind the method is based on the concepts of <i>weak learners</i> and <i>strong learners</i> from the subfield in Statistical Learning on <i>PAC learnability</i>, here PAC stands for <a href=\"https://en.wikipedia.org/wiki/Probably_approximately_correct_learning\">Probably Approximately Correct</a>. We touch lightly on the theory now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22ef0f8",
   "metadata": {},
   "source": [
    "### Weak learner vs strong learner\n",
    "\n",
    "Very roughly, a statistical learning algorithm is referred to as <i>weak learner</i> if it does slightly better than random guessing and is called a <i>strong learner</i> if it can be made arbitrarily close to the true relationship (given enough data).  These are the \"under-fitting models\" which we will ensemble through boosting.\n",
    "\n",
    "Making a weak learner is much easier than producing a strong learner in general, however, it has been shown that if a problem is weak learnable (meaning that a weak learner exists) then it is strong learnable (meaning that a strong learner exists). The fact that one exists led to the creation of algorithms and techniques to produce a strong learner. This is where boosting comes into play. \n",
    "\n",
    "In practice, a common weak learner algorithm is a <i>decision stump</i>, which is a decision tree of depth $1$.\n",
    "\n",
    "##### Regression or classification\n",
    "\n",
    "Boosting can be used for regression or classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c4f5ab",
   "metadata": {},
   "source": [
    "### Two specific boosting algorithms\n",
    "\n",
    "We will cover three boosting algorithms:\n",
    "1. Adaptive Boosting\n",
    "2. Gradient Boosting\n",
    "3. XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca96712",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "\n",
    "This notebook was written for the Erd&#337;s Institute C&#337;de Data Science Boot Camp by Matthew Osborne, Ph. D., 2023.  Modified by Steven Gubkin 2024.\n",
    "\n",
    "Any potential redistributors must seek and receive permission from Matthew Tyler Osborne, Ph.D. prior to redistribution. Redistribution of the material contained in this repository is conditional on acknowledgement of Matthew Tyler Osborne, Ph.D.'s original authorship and sponsorship of the Erd≈ës Institute as subject to the license (see License.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
