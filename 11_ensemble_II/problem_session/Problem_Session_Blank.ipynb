{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b59137f5",
   "metadata": {},
   "source": [
    "# Problem Session 11\n",
    "## A Concrete Strength Regression problem using Ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92da2294",
   "metadata": {},
   "outputs": [],
   "source": [
    "## For data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## This sets the plot style\n",
    "## to have a grid on a dark background\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e825338",
   "metadata": {},
   "source": [
    "We continue to work with the concrete compressive strength dataset from last week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a15757c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../data/concrete.csv')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train, df_test = train_test_split(df, \n",
    "                                                    shuffle=True,\n",
    "                                                    random_state=216,\n",
    "                                                    test_size=.2)\n",
    "df_tt, df_val = train_test_split(df_train, \n",
    "                                                    shuffle=True,\n",
    "                                                    random_state=216,\n",
    "                                                    test_size=.2)\n",
    "\n",
    "features = df.columns[:-1]\n",
    "target = df.columns[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d489d83",
   "metadata": {},
   "source": [
    "##### Model Selection\n",
    "\n",
    "Train each of the following models on the training set using their default parameters. Which has the smallest \"out of the box\" mean squared error?\n",
    "\n",
    "* Linear Regression\n",
    "* kNN\n",
    "* Support Vector Machine Regressor\n",
    "* Random Forest Regressor\n",
    "* AdaBoost Regressor\n",
    "* Gradient Boosting Regressor\n",
    "* XGBoost Regressor\n",
    "\n",
    "Hint: It is inefficient to copy/paste the four to five lines of code needed to train each model.\n",
    "\n",
    "I suggest instead storing the instantiated models in a dictionary and using a `for` loop!  You can, of course, use another method if you have a different preference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2072efc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1157e370",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d437f2f",
   "metadata": {},
   "source": [
    "##### Hyperparameter tuning and test set evaluation\n",
    "\n",
    "Select the model which had the lowest RMSE \"right out of the box\".  Do cross validation hyperparameter tuning on the combined training and validation set.  Note that if you make your grid of hyperparameters too large it might take a very long time to run.\n",
    "\n",
    "Once you obtain the hyperparameters with the best cross validation performance, train the model with those hyperparameters on the combined training and validation set.\n",
    "\n",
    "Evaluate performance on the test set.  Is it comparable to your training set performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad81e3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "02c4da73",
   "metadata": {},
   "source": [
    "Discussion Prompt:  A construction company is building a bridge and contracted out the specs to an engineer.  The engineer told them they needed to ensure that the concrete compressive strength is at least $50 \\textrm{ MPa}$ given the design constraints.  According to your model, the particular mix they are using is predicted to be $60 \\textrm{ MPa}$.\n",
    "\n",
    "Discuss this situation from a technical, ethical, and legal perspective.  Who else would you want to loop into this conversation?  \n",
    "\n",
    "Do a little further model assessment to see what the risk is.  For example, are there any instances where the model predicted strength in excess of $55 \\textrm{ MPa}$ but the actual strength was less than $50 \\textrm{ MPa}$?  What else could you do to assess the risk here?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa3901f",
   "metadata": {},
   "source": [
    "##### A more interpretable model?\n",
    "\n",
    "What if we care more about interpretability than we do about making the best predictive model?  One option is to use a generalized additive model (GAM).  Here we use `ExplainableBoostingRegressor` from `interpret` which you can read about [here](https://interpret.ml/docs/ebm.html).  Essentially it is a GAM where each additive component function is learned using gradient boosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa570b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from interpret.glassbox import ExplainableBoostingRegressor\n",
    "from interpret import show\n",
    "\n",
    "ebm = ExplainableBoostingRegressor(interactions=0)\n",
    "\n",
    "# Note: it is not good practice to evaluate multiple models on the test set.  \n",
    "# I am only doing so here to showcase the \"out of the box\" performance of this new model.\n",
    "\n",
    "ebm.fit(df_train[features], df_train[target])\n",
    "root_mean_squared_error(df_test[target], ebm.predict(df_test[features]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc9ebff",
   "metadata": {},
   "source": [
    "The model is not quite as good in terms of generalization capability (it is close!), but it is more interpretable.  Discuss any insights you gain from the following graphs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e074b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "show(ebm.explain_global())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540b3b02",
   "metadata": {},
   "source": [
    "I suspect the above will have taken the full hour.  If not, here are some additional questions!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b124e722",
   "metadata": {},
   "source": [
    "### More Questions about Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd23511",
   "metadata": {},
   "source": [
    "1. Give an example of a model which would benefit from boosting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff613d59",
   "metadata": {},
   "source": [
    "2. What happens if you use gradient boosting for linear regression with mean squared error loss?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4906e5",
   "metadata": {},
   "source": [
    "3. Code your own `CustomGradientBoostingRegressor` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53cf8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomGradientBoostingRegressor():\n",
    "    '''\n",
    "    Trains a sequence of regressors.  \n",
    "    Each new regressor has targets which are the residuals of the previous regressor.\n",
    "    Prediction is performed by summing the predictions of each individual regressor.\n",
    "    This is only designed to work with MSE loss.\n",
    "   '''\n",
    "    def __init__(self, base_estimator, num_estimators = 10, kwargs = None):\n",
    "        '''\n",
    "            Parameters:\n",
    "                base_estimator: An sklearn regression class.\n",
    "                num_estimators: The number of estimators in the ensemble\n",
    "                kwargs:  A dictionary of key word arguments to pass to the estimators.\n",
    "            Attributes:\n",
    "                self.estimators is a list of estimators instantiated with their kwargs\n",
    "        '''\n",
    "        self.base_estimator = \n",
    "        self.num_estimators = \n",
    "        self.kwargs = kwargs if kwargs else {}\n",
    "        self.estimators = \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "\n",
    "    def predict(self, X):\n",
    "        preds = \n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7863f80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure that your class is able to run the following code.\n",
    "# Does increasing the number of estimators decrease the MSE?\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X = np.random.normal(size = (100,2))\n",
    "y = X[:,0]**2 + X[:,0]*X[:,1]\n",
    "\n",
    "model =  CustomGradientBoostingRegressor(\n",
    "            base_estimator = DecisionTreeRegressor, \n",
    "            num_estimators = 1000, \n",
    "            kwargs = {'max_depth' : 1}\n",
    "            )\n",
    "\n",
    "model.fit(X,y)\n",
    "\n",
    "mean_squared_error(y, model.predict(X))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erdos_spring_2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
