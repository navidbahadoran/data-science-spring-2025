{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalized Linear Models\n",
    "\n",
    "We have devoted a great deal of attention to the linear model:\n",
    "\n",
    "$$Y \\sim \\mathcal{N}(x \\cdot \\beta, \\sigma^2)$$\n",
    "\n",
    "In lecture we also learned about logistic regression, which can be framed in the following form:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "Y \\sim \\textrm{Bernoulli}(p)\\\\\n",
    "\\log\\left( \\frac{1-p}{p}\\right) = x \\cdot \\beta\n",
    "\\end{align*} \n",
    "$$\n",
    "\n",
    "Note that in this case $\\mathbb{E}(Y|x) = p$.\n",
    "\n",
    "We can see that in both cases, some function of the conditional outcome means is a linear function of the predictor variables, i.e.\n",
    "\n",
    "$$\n",
    "g\\left(\\mathbb{E}(Y|x)\\right) = x \\cdot \\beta\n",
    "$$\n",
    "\n",
    "We also make an assumption on how $(Y|x)$ is distributed:  normal in one case, Bernoulli in the other.  In both cases, these distributions are part of what are known as *exponential families*.  These families of distributions include most of the named distributions you are probably familiar with.  We will see that the maximum likelihood estimation of the parameters is particularly nice for these distributions as well.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural Exponential Families\n",
    "\n",
    "Let $h:\\mathbb{R}^2 \\to [0,\\infty)$.  The **natural exponential family** of distributions associated with $h$ is a collection of distributions parameterized by two real numbers $\\theta$ and $\\phi$.  The probability density function of the distribution $y_{\\theta, \\phi}$ is\n",
    "\n",
    "$$\n",
    "f_Y(y;\\theta) = \\exp\\left(y\\theta - A(\\theta)\\right)h(y)\n",
    "$$\n",
    "\n",
    "where $A(\\theta)$ is chosen to make the integral with respect to $y$ equal to $1$:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "1 &= \\int_{-\\infty}^{\\infty} \\exp\\left( y\\theta - A(\\theta)\\right)h(y) \\mathop{dy}\\\\\n",
    "\\exp\\left(A(\\theta)\\right) &= \\int_{-\\infty}^{\\infty} \\exp\\left( y\\theta\\right)h(y) \\mathop{dy}\\\\\n",
    "A(\\theta) &= \\log \\int_{-\\infty}^{\\infty} \\exp\\left(y\\theta\\right)h(y) \\mathop{dy}\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Some terminology:\n",
    "\n",
    "* We call $\\theta$ the *natural parameter*.\n",
    "* We call $h(y)$ the base density. \n",
    "* We call $A$ the *cumulant function* or *log-partition function*.\n",
    "\n",
    "Note:  We are sacrificing some generality for the sake of clarity here.  More generally, an exponential family of distributions has the form \n",
    "\n",
    "$$\n",
    "f_Y(y ; \\theta) = \\exp(T(y) \\cdot \\eta(\\theta) - A(\\theta)) h(y)\n",
    "$$\n",
    "\n",
    "where $\\theta \\in \\mathbb{R}^s$, $\\eta: \\mathbb{R}^s \\to \\mathbb{R}^d$ and $T:\\mathbb{R} \\to \\mathbb{R}^d$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example:  Normal distributions with known variance\n",
    "\n",
    "Say that the variance $\\sigma^2$ is known and consider the family\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "f_Y(y) &= \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left( -\\frac{(y-\\mu)^2}{2\\sigma^2}\\right)\\\\\n",
    "&= \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left( -\\frac{y^2 - 2y\\mu +\\mu^2}{2\\sigma^2}\\right)\\\\\n",
    "&= \\exp\\left(y (\\frac{\\mu}{\\sigma^2}) - \\frac{1}{2}\\sigma^2 (\\frac{\\mu}{\\sigma^2})^2\\right) \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left( -\\frac{y^2}{2\\sigma^2}\\right)\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "So the normal distributions with a known variance form a natural exponential family with \n",
    "\n",
    "* $\\theta = \\mu/\\sigma^2$\n",
    "* $h(y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left( -\\frac{y^2}{2\\sigma^2}\\right)$\n",
    "* $A(\\theta) = \\frac{1}{2}\\sigma^2 \\theta^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spine surgery \n",
    "216-444-1889"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Bernoulli distributions\n",
    "\n",
    "A discrete random variable $Y$ is Bernoulli distributed when $P(Y = 1) = p$ and $P(Y = 0) = 1-p$.  We can rewrite the pmf as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "f_Y(y) &= p^y(1-p)^{1-y} \\textrm{ with $y \\in \\{0,1\\}$}\\\\\n",
    "&= \\exp(y\\log(p) + (1-y)\\log(1-p))\\\\\n",
    "&= \\exp(y\\log(\\frac{p}{1-p}) + \\log(1-p))\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "To write this in the form of an exponential family we need to express $-\\log(1-p)$ as a function of $\\theta = \\log(\\frac{p}{1-p})$.  Let's first solve for $p$ as a function of $\\theta$:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\theta &= \\log(\\frac{p}{1-p})\\\\\n",
    "e^\\theta &= \\frac{p}{1-p}\\\\\n",
    "e^{-\\theta} &= \\frac{1-p}{p}\\\\\n",
    "e^{-\\theta} &= \\frac{1}{p} - 1\\\\\n",
    "p &= \\frac{1}{1+e^{-\\theta}}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Thus \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "-\\log(1-p) &= -\\log(1 - \\frac{1}{1+e^{-\\theta}})\\\\\n",
    "&= -\\log(\\frac{e^{-\\theta}}{1+e^{-\\theta}})\\\\\n",
    "&= -\\log(\\frac{1}{1+e^{\\theta}})\\\\\n",
    "&= \\log(1+e^{\\theta})\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "So the Bernoulli distributions are a natural exponential family with \n",
    "\n",
    "* $\\theta = \\log(\\frac{p}{1-p})$\n",
    "* $h(y) = 1$\n",
    "* $A(\\theta) = \\log(1 + \\exp(\\theta))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Poisson distributions\n",
    "\n",
    "Say that some event occurs at irregular time intervals.  These events are mutually independent.  If the average number of events in a unit time interval is $\\lambda$, then it can be shown that the number of events $Y$ in any unit time interval follows a [Poisson distribution](https://en.wikipedia.org/wiki/Poisson_distribution) with pmf:\n",
    "\n",
    "$$\n",
    "f_Y(y) = \\frac{1}{y!} \\lambda^ye^{-\\lambda} \\textrm{ where $y \\in \\{0,1,2,3,\\dots\\}$}\n",
    "$$\n",
    "\n",
    "Let's try to write this in the form of a natural exponential family:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{1}{y!} \\lambda^ye^{-\\lambda} &= \\frac{1}{y!} \\exp(y \\log(\\lambda) - \\lambda )\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "So the Poisson distributions form a natural exponential family with\n",
    "\n",
    "* $\\theta = \\log(\\lambda)$\n",
    "* $h(y) = \\frac{1}{y!}$\n",
    "* $A(\\theta) = e^\\theta$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results about the cumulant function $A(\\theta)$\n",
    "\n",
    "We will prove that for a natural exponential family, $A'(\\theta) = \\mathbb{E}(Y)$ and $A''(\\theta) = \\operatorname{Var}(Y)$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\exp\\left(A(\\theta)\\right) &= \\int_{-\\infty}^{\\infty} \\exp\\left( y\\theta\\right)h(y) \\mathop{dy}\\\\\n",
    "\\frac{d}{d\\theta}\\exp\\left(A(\\theta)\\right) &= \\frac{d}{d\\theta}\\int_{-\\infty}^{\\infty} \\exp\\left( y\\theta\\right)h(y) \\mathop{dy}\\\\\n",
    "A'(\\theta) \\exp\\left(A(\\theta)\\right) &= \\int_{-\\infty}^{\\infty} y\\exp\\left( y\\theta\\right)h(y) \\mathop{dy}\\\\\n",
    "A'(\\theta) &= \\int_{-\\infty}^{\\infty} y\\exp\\left( y\\theta - A(\\theta)\\right)h(y) \\mathop{dy}\\\\\n",
    "A'(\\theta) &= \\int_{-\\infty}^{\\infty} yf_Y(y)\\mathop{dy}\\\\\n",
    "A'(\\theta) &= \\mathbb{E}(y)\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Starting at line (3) and differentiating with respect to $\\theta$ again we have\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{d}{d\\theta} \\left[A'(\\theta) \\exp\\left(A(\\theta)\\right) \\right] &= \\frac{d}{d\\theta}\\int_{-\\infty}^{\\infty} y\\exp\\left( y\\theta\\right)h(y) \\mathop{dy}\\\\\n",
    "A''(\\theta) \\exp\\left(A(\\theta)\\right) + A'(\\theta)^2\\exp\\left(A(\\theta)\\right)  &= \\int_{-\\infty}^{\\infty} y^2\\exp\\left( y\\theta\\right)h(y) \\mathop{dy}\\\\\n",
    "A''(\\theta) + (\\mathbb{E}(Y))^2 &= \\mathbb{E}(Y^2)\\\\\n",
    "A''(\\theta) &= \\operatorname{Var}(Y) \n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Note also that $A''(\\theta) > 0$ since variance is positive.  This also implies that $A'$ is an increasing function, and thus has a well defined inverse.\n",
    "\n",
    "**Definition**: We call $g = (A')^{-1}$ the **canonical link function** of the natural exponential family."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalized Linear Models\n",
    "\n",
    "**Definition**:  A **Generalized Linear Model with canonical link** is specified by choosing a natural exponential family for the target variable. We make the assumption that \n",
    "\n",
    "$$\n",
    "g(\\mathbb{E}(Y)) = \\textbf{x} \\cdot \\beta\n",
    "$$\n",
    "\n",
    "where $g$ is the canonical link function of the family.\n",
    "\n",
    "Note that since $g$ is the canonical link, we can also say that\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\textbf{x} \\cdot \\beta \n",
    "&= g(\\mathbb{E}(Y))\\\\\n",
    "&= g(A'(\\theta))\\\\\n",
    "&= \\theta\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "In other words, when we are using a canonical link, the assumption we are making when we specify a GLM is that\n",
    "\n",
    "$$\n",
    "f_Y(y) = \\exp(y \\textbf{x} \\cdot \\beta - A(\\textbf{x} \\cdot \\beta ))h(y)\n",
    "$$\n",
    "\n",
    "Note:  We are restricting ourselves to natural exponential families and canonical links for this notebook.  One can use more general links and more expansive definitions of exponential families, but the math gets a bit more complicated. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example:  Bernoulli Distrubution $\\to$ Logistic Regression\n",
    "\n",
    "As noted previously, the the Bernoulli distributions are a natural exponential family with \n",
    "\n",
    "* $\\theta = \\log(\\frac{p}{1-p})$\n",
    "* $h(y) = 1$\n",
    "* $A(\\theta) = \\log(1 + \\exp(\\theta))$\n",
    "\n",
    "Since $A'(\\theta) = \\frac{\\exp(\\theta)}{1 + \\exp(\\theta)} = \\frac{1}{1+\\exp(-\\theta)}$, the canonical link function is the inverse of $A'$ which is $g(u) = \\log(\\frac{u}{1-u})$.\n",
    "\n",
    "Thus the GLM with a Bernoulli distributed outcome and canonical link is specified by\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "Y &\\sim \\operatorname{Bernoulli}(p)\\\\\n",
    "\\log(\\frac{p}{1-p}) &= \\textbf{x} \\cdot \\beta\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "This is exactly Logistic Regression:  the log odds are linear in the predictors, and the response is Bernoulli distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example:  Poisson Distrubutions $\\to$ Poisson Regression\n",
    "\n",
    "The Poisson distributions form a natural exponential family with\n",
    "\n",
    "* $\\theta = \\log(\\lambda)$\n",
    "* $h(y) = \\frac{1}{y!}$\n",
    "* $A(\\theta) = e^\\theta$ \n",
    "\n",
    "So the GLM with canonical link $g(u) = \\log(u)$ is specified by\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "Y &\\sim \\operatorname{Poisson}(\\lambda)\\\\\n",
    "\\log(\\lambda) &= \\textbf{x} \\cdot \\beta\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "[This Desmos graph](https://www.desmos.com/3d/yiiihjiudp) might help you to visualize what is going on with Poisson Regression.  This form of regression is often used when the outcome is a count."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:  Unfortunately vanilla OLS doesn't fit exactly into the simplified framework we are addressing in this notebook.  We would need to introduce an additional dispersion parameter into our definition of \"generalized linear model\" to get it to work out nicely.  You can try using the normal distribution with fixed variance, and you will get something which is very close to OLS linear regression but with some annoying additional factors of $\\sigma^2$ sprinkled around.  Essentially, it models not the mean conditional response, but this mean conditional response when scaled down by a factor of $\\sigma^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLE estimation of GLMs with canonical links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a GLM, how can we use data to estimate the model parameters $\\beta$?\n",
    "\n",
    "\n",
    "Say that we have fixed predictors $\\textbf{x}_i$ for $i=1,2,3, \\dots, n$.  Let the corresponding random variable for the outcome be denoted $Y_i$.  Let $y_i$ be the observed outcomes from these distributions.\n",
    "\n",
    "The likelihood function is then\n",
    "\n",
    "$$\n",
    "L(\\beta) = \\prod_1^n f_{Y_i}(y_i) = \\prod_1^n \\exp(y_i \\textbf{x}_i \\cdot \\beta - A(\\textbf{x}_i \\cdot \\beta))h(y_i)\n",
    "$$\n",
    "\n",
    "Then the negative log likelihood is\n",
    "\n",
    "$$\n",
    "\\ell(\\beta) = \\sum_1^n \\left(A(\\textbf{x}_i \\cdot \\beta) - y_i \\textbf{x}_i \\cdot \\beta\\right) - \\sum_1^n \\log(h(y_i))\n",
    "$$\n",
    "\n",
    "We want to compute the gradient and Hessian of $\\ell$.\n",
    "\n",
    "Taking the partial derivative with respect to $\\beta_j$ we obtain\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\partial_j \\ell \n",
    "&= \\sum_1^n A'(\\textbf{x}_i \\cdot \\beta) \\textbf{x}_{ij} - y_i \\textbf{x}_{ij}\\\\\n",
    "&= \\sum_1^n \\left(g^{-1}(\\textbf{x}_i \\cdot \\beta)  - y_i \\right)\\textbf{x}_{ij}\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "so, packaging the $\\textbf{x}_i$ into a matrix $X$ and $y_i$ into a vector $\\vec{y}_{\\textrm{obs}}$ as usual, we have\n",
    "\n",
    "$$\n",
    "\\nabla \\ell = X^\\top (g^{-1}(X \\beta) - \\vec{y}_{\\textrm{obs}})\n",
    "$$\n",
    "\n",
    "where $g^{-1}$ is being applied coordinate wise.\n",
    "\n",
    "Introducing the notation $g^{-1}(X \\beta) = \\hat(y)$ (reasonable since it is the vector of predicted outcome means), we can write this as \n",
    "\n",
    "$$\n",
    "\\nabla \\ell = X^\\top (\\hat{y} - \\vec{y}_{\\textrm{obs}})\n",
    "$$\n",
    "\n",
    "In other words, \"The gradient of the negative log likelihood is $X^\\top$ applied to the deviations of the observed outcomes from the predicted outcome means\".  This simple form of the gradient makes gradient descent super easy to implement for fitting GLMs.\n",
    "\n",
    "In the case of Logistic Regression this is $\\nabla \\ell = X^\\top (\\sigma (X\\beta) - \\vec{y}_{\\textrm{obs}})$ as claimed in the lecture notebook!\n",
    "\n",
    "For Poisson regression it would be $\\nabla \\ell = X^\\top (\\exp (X\\beta) - \\vec{y}_{\\textrm{obs}})$.\n",
    "\n",
    "We can also obtain the Hessian\n",
    "\n",
    "$\\nabla^2 \\ell = X^\\top \\operatorname{diag}(A''(X\\beta)) X$\n",
    "\n",
    "which is manifestly positive definite.  This shows that the fitting the GLM through MLE is a smooth convex optimization problem, which gives us uniqueness of the minimizing parameters (assuming they exist).  Note that it is possible that $\\ell$ has no global minimum, as occurs in the case of perfect separation in Logistic Regression.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erdos_spring_2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
