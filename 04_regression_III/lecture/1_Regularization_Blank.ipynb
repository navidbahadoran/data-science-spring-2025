{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bd7c765",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "\n",
    "Regularization involves adding a penalty term to our loss function. It turns out that this penalty term can help combat overfitting by making the model more biased but with less variance.\n",
    "\n",
    "## What we will accomplish\n",
    "\n",
    "In this notebook we will:\n",
    "- Introduce the general idea behind regularization,\n",
    "- Discuss ridge and lasso regression as particular regularization algorithms\n",
    "- Discuss how ridge can combat multicollinearity.\n",
    "- Show how lasso is nice for feature selection.\n",
    "\n",
    "##### Quick Note\n",
    "\n",
    "This notebook is a little math heavy, I will do my best to provide both mathematical insight for those that want it and give a broad overview for those that do not want to delve too much into the math specifics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d62ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the packages we'll use\n",
    "## For data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import meshgrid\n",
    "\n",
    "## For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## This sets the plot style\n",
    "## to have a grid on a white background\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60124ec",
   "metadata": {},
   "source": [
    "## Coefficient explosions\n",
    "\n",
    "Let's return to our example from the `Bias-Variance Tradeoff` notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8849b5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(216)\n",
    "## Generate data\n",
    "x = np.linspace(-3,3,100)\n",
    "y = x*(x-1) + 1.2*rng.standard_normal(100)\n",
    "\n",
    "\n",
    "## plot the data alongside the true relationship\n",
    "plt.figure(figsize = (6,6))\n",
    "\n",
    "plt.scatter(x,y, label=\"Observed Data\")\n",
    "\n",
    "plt.plot(x,x*(x-1),'k', label=\"True Relationship\")\n",
    "\n",
    "plt.xlabel(\"x\",fontsize=12)\n",
    "plt.ylabel(\"y\",fontsize=12)\n",
    "\n",
    "plt.legend(fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd562b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import the functions/objects we'll need\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee4cbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make an array of zeros that will hold some data for me\n",
    "n = 26\n",
    "coef_holder = np.zeros((n,n))\n",
    "\n",
    "## Now we'll fit the data with polynomials degree 1 through n\n",
    "for i in range(1,n+1):\n",
    "    ## Make a pipe\n",
    "    pipe = Pipeline([('poly',PolynomialFeatures(i,include_bias = False)),\n",
    "                    ('reg',LinearRegression())])\n",
    "    \n",
    "    ## fit the data\n",
    "    pipe.fit(x.reshape(-1,1),y)\n",
    "    \n",
    "    ## store the coefficient estimates\n",
    "    coef_holder[i-1,:i] = np.round(pipe['reg'].coef_,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b2d633",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Display the coefficient estimates as a dataframe\n",
    "pd.DataFrame(coef_holder, \n",
    "             columns = [\"x^\" + str(i) for i in range(1,n+1)],\n",
    "            index = [str(i) + \"_deg_poly\" for i in range(1,n+1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a657f3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_norms = []\n",
    "\n",
    "for i in range(n):\n",
    "    beta_norms.append(np.linalg.norm(coef_holder[i,:]))\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "\n",
    "plt.plot(range(1,n+1), beta_norms)\n",
    "plt.ylabel(\"Length of $\\hat{\\\\beta}$\", fontsize=12)\n",
    "plt.xlabel(\"Degree of polynomial\", fontsize=12)\n",
    "plt.xticks(fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d754519",
   "metadata": {},
   "source": [
    "Looking at the dataframe we have just produced we can notice that a number of our coefficients get larger in magnitude as the model gets more complex. \n",
    "\n",
    "This observation leads to the main idea behind regularization.\n",
    "\n",
    "## The idea behind regularization\n",
    "\n",
    "Suppose the non-intercept coefficients from the regression are denoted by $\\beta$, i.e. $\\beta=\\left(\\beta_1,\\beta_2,\\dots,\\beta_p\\right)^T$. Recall that in Ordinary Least Squares regression our goal is to estimate $\\beta$ so that\n",
    "\n",
    "$$\n",
    "MSE = \\frac{1}{n}(y - X\\beta - \\beta_0)^T(y - X\\beta - \\beta_0)\n",
    "$$\n",
    "\n",
    "is minimized on the training data. <i>Note here that I have separated the intercept $\\beta_0$ from the remaining coefficients, so $X$ is not assumed to have a column of $1$ in this notebook.</i>\n",
    "\n",
    "The main idea behind regularization is to still minimize the MSE, BUT while also ensuring that $\\beta$ doesn't get too large. \n",
    "\n",
    "#### Penalizing large parameters\n",
    "\n",
    "We can modify our loss function to penalize \"large\" parameters. \n",
    "\n",
    "$$\n",
    "||y-X\\beta - \\beta_0||^2_2 + \\alpha\\operatorname{Size}(\\beta)\n",
    "$$\n",
    "\n",
    "Here $\\alpha$ is an adjustable constant which we will call a <i>hyperparameter</i>: a parameter we set before fitting the model.  Different ways of measuring the size of a vector lead to different regularization methods.\n",
    "\n",
    "For $\\alpha=0$ we recover the OLS estimate for $\\beta$, for $\\alpha=\\infty$ we get $\\beta=0$, values of $\\alpha$ between those two extremes will give different coefficient estimates. The value of $\\alpha$ that gives the best model for your data depends on the problem and can be found through cross-validation model comparisons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba779f77",
   "metadata": {},
   "source": [
    "## Specific regularization models\n",
    "\n",
    "<i>Ridge regression</i> and <i>lasso</i> are two forms of regularization where we make specific choices of how to measure the \"size\" of the parameters.\n",
    "\n",
    "### Ridge regression\n",
    "\n",
    "In ridge regression we use the the size of $\\beta$ as the square of the Euclidean length (or \"$\\ell_2$-norm\") of $\\beta$:\n",
    "\n",
    "$$\n",
    "\\operatorname{Size}_{\\textrm{Ridge}}(\\beta) = ||\\beta||^2_2 = \\beta_1^2 + \\beta_2^2 + \\dots + \\beta_p^2.\n",
    "$$\n",
    "\n",
    "This is the length you get using the Pythogorean Theorem!\n",
    "\n",
    "### Lasso regression\n",
    "\n",
    "In lasso regression we take $\\operatorname{Size}(\\beta)$ to be the $\\ell_1$-norm:\n",
    "\n",
    "$$\n",
    "\\operatorname{Size}_{\\textrm{Lasso}}(\\beta)  = ||\\beta||_1 = |\\beta_1| + |\\beta_2| + \\dots + |\\beta_p|.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc4d221",
   "metadata": {},
   "source": [
    "### Some geometric intuition\n",
    "\n",
    "<p float=\"left\">\n",
    "  <img src=\"lecture_assets/ridge.png\" width=\"600\" />\n",
    "  <img src=\"lecture_assets/lasso.png\" width=\"618\" />\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2abe634",
   "metadata": {},
   "source": [
    "On the left we see a picture of contour lines (in blue) for the MSE of a particular linear regression problem with two features.  The purple circles are the contours of the $\\ell_2$ norm.  The OLS estimate, which minimizes the MSE alone, is at the center of all of the ellipses.  The origin minimizes the $\\ell_2$ norm.  Ridge regression gives us a \"tug of war\" between these two quantities. Note that a ridge regression solution $\\hat{\\beta}_\\alpha$ must be at a point of tangency:  if it were at a point of transverse intersection between contours you could move along one contour while decreasing the other.  We can see some different ridge estimates here:  the small $\\alpha$ are close to the OLS estimates, while large $\\alpha$ is close to the origin.\n",
    "\n",
    "We get a similar picture for Lasso regression, but the contours of the $\\ell_1$ norm are squares instead of circles!  The same argument about tangency applies *until* we intersect a coordinate axis.  Notice that it is clear, from this picture, that the $\\hat{\\beta}_\\alpha$ must follow a **piecewise linear** path as we vary $\\alpha$ (a fact which has some cool applications)!\n",
    "\n",
    "We can also see that while Ridge regression will never zero out a parameter, Lasso will!  In this way, Lasso can be used for \"automatic feature selection\". \n",
    "\n",
    "Let's see this play out by fitting a degree $10$ polynomial our data using both Ridge and Lasso regression and seeing how the coefficients change as we adjust $\\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d3709b",
   "metadata": {},
   "source": [
    "### Implementing in `sklearn`\n",
    "\n",
    "We can implement both of these models in `sklearn` with `Ridge` <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html\">https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html</a> for ridge regression and `Lasso` <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html\">https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html</a> for lasso regression.\n",
    "\n",
    "<i>Note: ridge and lasso regression are examples of algorithms/models where scaling the data is a step that should be taken prior to fitting the model. This is because vastly different scales can impact the scales of the components of $\\beta$. This can make it so that there is not enough room in the $\\beta$-budget to afford the actual values of the individual coefficients.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed3776d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import the models here\n",
    "## Ridge and Lasso regression are stored in linear_model\n",
    "\n",
    "# import models here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e622ba19",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This code will allow us to demonstrate the effect of \n",
    "## increasing alpha\n",
    "\n",
    "## set values for alpha\n",
    "alpha = [0.00001,0.0001,0.001,0.01,0.1,1,10,100,1000]\n",
    "\n",
    "## The degree of the polynomial we will fit\n",
    "n=10\n",
    "\n",
    "#$ These will hold our coefficient estimates\n",
    "ridge_coefs = np.empty((len(alpha),n))\n",
    "lasso_coefs = np.empty((len(alpha),n))\n",
    "\n",
    "## for each alpha value\n",
    "for i in range(len(alpha)):\n",
    "    ## set up the ridge pipeline\n",
    "    ## first scale\n",
    "    ## then make polynomial features\n",
    "    ## then fit the ridge regression model\n",
    "    ridge_pipe = Pipeline([('scale',StandardScaler()),\n",
    "                              ('poly',PolynomialFeatures(n, interaction_only=False, include_bias=False))\n",
    "                              # ridge model here, max_iter=5000000,\n",
    "                              ])\n",
    "    \n",
    "    ## set up the lasso pipeline\n",
    "    ## same steps as with ridge\n",
    "    lasso_pipe = Pipeline([('scale',StandardScaler()),\n",
    "                              ('poly',PolynomialFeatures(n, interaction_only=False, include_bias=False)),\n",
    "                              # lasso model here, max_iter=5000000,\n",
    "                          ])\n",
    "    \n",
    "    ## fit the ridge\n",
    "    ridge_pipe.fit(x.reshape(-1,1), y)\n",
    "    \n",
    "    ## fit the lasso\n",
    "    lasso_pipe.fit(x.reshape(-1,1), y)\n",
    "\n",
    "    \n",
    "    # record the coefficients\n",
    "    ridge_coefs[i,:] = ridge_pipe['ridge'].coef_\n",
    "    lasso_coefs[i,:] = lasso_pipe['lasso'].coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc52e810",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Ridge Coefficients\")\n",
    "\n",
    "pd.DataFrame(np.round(ridge_coefs,8),\n",
    "            columns = [\"x^\" + str(i) for i in range(1,n+1)],\n",
    "            index = [\"alpha=\" + str(a) for a in alpha])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc4fe6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Lasso Coefficients\")\n",
    "\n",
    "pd.DataFrame(np.round(lasso_coefs,8),\n",
    "            columns = [\"x^\" + str(i) for i in range(1,n+1)],\n",
    "            index = [\"alpha=\" + str(a) for a in alpha])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978dda9c",
   "metadata": {},
   "source": [
    "### An important note about scaling.\n",
    "\n",
    "OLS linear regression is scale invariant.  That means that if you scale your features and run OLS linear regression you will get exactly the same predictions as if you had not scaled them.  Another way of thinking about this is that OLS will give you the same results no matter what units you use for your features:  for example if $x_1$ is height in meters and $x_1'$ is height in centimeters, then the OLS estimates $\\hat{\\beta}_1$ and $\\hat{\\beta}_1'$ would be related by $\\hat{\\beta}_1' = 100 \\hat{\\beta}_1$.  Changing the unit from m to cm just changes the unit of $\\beta$ from $\\frac{\\textrm{units}}{m}$ to $\\frac{\\textrm{units}}{cm}$.\n",
    "\n",
    "Both Ridge and Lasso regression **are not** scale invariant.  It is easy to see why: if we change from $m$ to $cm$ the \"size\" the OLS parameters will change by a factor of $\\frac{1}{100}$.  As a consequence, both Ridge and Lasso will prioritize keeping these predictors in the model, since the coefficient is not very \"expensive\" in terms of parameter size but does a lot to decrease the MSE.\n",
    "\n",
    "To avoid this it is highly advisable to *scale and center your data* before Ridge or Lasso regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6ec113",
   "metadata": {},
   "source": [
    "## Which one to use?\n",
    "\n",
    "Which algorithm is the better choice? Well that depends on the problem. Both are good at addressing overfitting concerns, but each has a couple unique pros and cons.\n",
    "\n",
    "##### Lasso\n",
    "\n",
    "<b>Pros</b>\n",
    "\n",
    "- Works well when you have a large number of features that do not have any effect on the target\n",
    "- Feature selection is a plus, this can allow for a sparser model which is good for computational reasons.\n",
    "- Feature selection can also produce a more interpretable model.\n",
    "\n",
    "<b>Cons</b>\n",
    "\n",
    "- Can have trouble with highly correlated features (colinearity), it typically chooses one variable among those that are correlated, which may be random.\n",
    "\n",
    "##### Ridge\n",
    "\n",
    "<b>Pros</b>\n",
    "\n",
    "- Works well when the target depends on all or most of the features and\n",
    "- Can handle colinearity better than lasso.\n",
    "\n",
    "<b>Cons</b>\n",
    "\n",
    "- Because ridge typically keeps most of the predictors in the model, this can be a computationally costly model type for data sets with a large number of predictors.\n",
    "- Keeping all features also makes interpretation of the model difficult.\n",
    "\n",
    "##### Elastic Net\n",
    "\n",
    "Sometimes the best model will be something in between ridge and lasso. This technique is known as <i>elastic net</i> and will be demonstrated in a `Practice Problems` notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9d9056",
   "metadata": {},
   "source": [
    "## Notebook specific references\n",
    "\n",
    "To help teach this lesson I consulted some additional source I found through a Google search. Here are links to those references for you to take a deeper dive into ridge and lasso regression.\n",
    "\n",
    "1. <a href=\"https://www.statlearning.com/\">https://www.statlearning.com/</a>\n",
    "2. <a href=\"https://www.analyticsvidhya.com/blog/2016/01/ridge-lasso-regression-python-complete-tutorial/\">https://www.analyticsvidhya.com/blog/2016/01/ridge-lasso-regression-python-complete-tutorial/</a>\n",
    "3. <a href=\"https://suzyahyah.github.io/optimization/2018/07/20/Constrained-unconstrained-form-Ridge.html\">https://suzyahyah.github.io/optimization/2018/07/20/Constrained-unconstrained-form-Ridge.html</a>\n",
    "4. <a href=\"https://statweb.stanford.edu/~owen/courses/305a/Rudyregularization.pdf\">https://statweb.stanford.edu/~owen/courses/305a/Rudyregularization.pdf</a>\n",
    "5. <a href=\"http://web.mit.edu/zoya/www/linearRegression.pdf\">http://web.mit.edu/zoya/www/linearRegression.pdf</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2020ee60",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "\n",
    "This notebook was written for the Erd&#337;s Institute C&#337;de Data Science Boot Camp by Matthew Osborne, Ph. D., 2023.  Modified by Steven Gubkin 2024.\n",
    "\n",
    "Any potential redistributors must seek and receive permission from Matthew Tyler Osborne, Ph.D. prior to redistribution. Redistribution of the material contained in this repository is conditional on acknowledgement of Matthew Tyler Osborne, Ph.D.'s original authorship and sponsorship of the Erdős Institute as subject to the license (see License.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
