{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "704aaa59",
   "metadata": {},
   "source": [
    "# Principal Components Analysis (PCA)\n",
    "\n",
    "## What we will accomplish\n",
    "\n",
    "In this notebook we will:\n",
    "- Learn the concept behind PCA,\n",
    "- Review the mathematics underlying PCA,\n",
    "- Show how to implement PCA in `sklearn`,\n",
    "- See how we can attempt to interpret the results of PCA.\n",
    "- Discuss when it is appropriate or inappropriate to use PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5778fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## For data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from seaborn import set_style\n",
    "\n",
    "## This sets the plot style\n",
    "## to have a grid on a dark background\n",
    "set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b26ef0",
   "metadata": {},
   "source": [
    "## The intuition behind PCA\n",
    "\n",
    "PCA looks to reduce the dimension of a data set by projecting the data  onto a lower dimensional space that captures as much of the original variance as possible. Thinking in terms of optimization, your goal is to project into a lower dimensional space in a way that maximizes variance.\n",
    "\n",
    "Here is a heuristic algorithm:\n",
    "1. Center your data so that each feature has 0 mean, this is done for convenience.\n",
    "2. Find the direction in space along which projections have the highest variance, this produces the first principal component.\n",
    "3. Find the direction orthogonal to the first principal component that maximizes variance, this is the second principal component.\n",
    "4. Continue in this way, the kth principal component is the variance-maximizing direction orthogonal to the previous k-1 components.\n",
    "\n",
    "Let's see what we mean in a 2-D example, we will use `sklearn`'s `PCA` object, <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\">https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html</a>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a29deeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make some random data\n",
    "rng = np.random.default_rng(216)\n",
    "\n",
    "x1 = 9*rng.standard_normal(500)\n",
    "x2 = 2*rng.standard_normal(500)\n",
    "\n",
    "X = np.concatenate([x1.reshape(-1,1),x2.reshape(-1,1)], axis = 1)\n",
    "\n",
    "angle = -np.pi/4\n",
    "\n",
    "X = X.dot(np.array([[np.cos(angle),-np.sin(angle)],[np.sin(angle),np.cos(angle)]]))\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(X[:, 0], X[:, 1])\n",
    "\n",
    "plt.xlabel(\"$x_1$\", fontsize=12)\n",
    "plt.ylabel(\"$x_2$\", fontsize=12)\n",
    "\n",
    "plt.axis('equal')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d957ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PCA is stored in decomposition\n",
    "\n",
    "# import here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59bea9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## make the PCA object\n",
    "## we'll project down to 2-D\n",
    "\n",
    "pca =  \n",
    "\n",
    "## Fit the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bd8be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc3d49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This function will draw a vector in 2D\n",
    "## given its components\n",
    "def draw_vector(v0, v1, ax=None):\n",
    "    ax = ax or plt.gca()\n",
    "    arrowprops=dict(arrowstyle='->',\n",
    "                    linewidth=2,\n",
    "                    shrinkA=0, \n",
    "                    shrinkB=0,\n",
    "                    color=\"black\")\n",
    "    ax.annotate('', v1, v0, arrowprops=arrowprops)\n",
    "\n",
    "# plot data\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(X[:, 0], X[:, 1], alpha=.4)\n",
    "\n",
    "for length, vector in zip(pca.explained_variance_, pca.components_):\n",
    "    v = vector * 3 * np.sqrt(length)\n",
    "    draw_vector(pca.mean_, pca.mean_ + v)\n",
    "    \n",
    "    \n",
    "plt.xlabel(\"$x_1$\", fontsize=12)\n",
    "plt.ylabel(\"$x_2$\", fontsize=12)\n",
    "\n",
    "plt.title(\"Original Data and Component Vectors\", fontsize=14)\n",
    "\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594cdcc4",
   "metadata": {},
   "source": [
    "The vectors drawn above are called the <i>component vectors</i> of the PCA. When we want to get the transformed version of the data, we get the scalar projection of each observation onto the component vectors. We'll visualize this more explicitly after reviewing the math behind PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2415682b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform gets you the PCA transformed values\n",
    "fit = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c345be6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "plt.scatter(fit[:,0], fit[:,1], alpha=.8)\n",
    "\n",
    "plt.xlabel(\"First PCA value\", fontsize=12)\n",
    "plt.ylabel(\"Second PCA value\", fontsize=12)\n",
    "\n",
    "plt.title(\"PCA Transformed Data\", fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607ab38c",
   "metadata": {},
   "source": [
    "## The math behind PCA \n",
    "\n",
    "Let $X$ be our design matrix, which we will assume has been centered for simplicity.\n",
    "\n",
    "Our goal is to find $\\vec{w}$ with $||w|| = 1$, such that $\\text{Var}( X \\vec{w})$ is maximized (note that because $||\\vec{w}||=1$, $Xw$ is a vector of scalar projections of the rows of $X$ onto $\\vec{w}$).  The following gif (from one of the [best stats.stackoverflow answers of all time](https://stats.stackexchange.com/a/140579/97124)) illustrates our goal geometrically:  we are looking for the direction $\\vec{w}$ which maximizes the variance of the projection into that direction.\n",
    "\n",
    "![SegmentLocal](lecture_assets/pca.gif)\n",
    "\n",
    "Because we have centered the columns of $X$ we have:\n",
    "\n",
    "$$\n",
    "\\text{Var}(Xw) = E(w^T X^T X w) = w^T E(X^T X) w = w^T \\Sigma w,\n",
    "$$\n",
    "\n",
    "where $\\Sigma$ is the covariance matrix of $X$.\n",
    "\n",
    "Our constrained optimization problem is now:\n",
    "\n",
    "$$\n",
    "\\text{optimize } f(w) = w^T \\Sigma w, \\text{ constrained to } g(w) = w^T w - 1 = 0.\n",
    "$$\n",
    "\n",
    "Using the method of Lagrange multipliers and some matrix calculus:\n",
    "\n",
    "$$\n",
    " \\nabla \\left(w^T \\Sigma w - \\lambda (w^T w - 1)\\right) = 2 \\Sigma w - 2\\lambda w.\n",
    "$$\n",
    "\n",
    "Setting this equal to $0$ and solving gives\n",
    "$$\n",
    "\\Sigma w = \\lambda w,\n",
    "$$\n",
    "the standard eigenvalue setup.\n",
    "\n",
    "So the vector $w$ that maximizes variance is an eigenvector corresponding to the largest eigenvalue of the covariance matrix of $X$.\n",
    "\n",
    "This vector is known as the first principal component $\\vec{v}_1$.\n",
    "\n",
    "The $j^{th}$ principle component $\\vec{v}_j$ is the direction which maximizes $\\operatorname{Var}(X\\vec{w})$ subject to $\\vec{w} \\in \\operatorname{Span}(\\vec{v}_1, \\vec{v}_2, \\dots, \\vec{v}_{j-1})^\\perp$ \n",
    "\n",
    "Note because $\\Sigma$ is an $p\\times p$ real positive symmetric matrix it has a set of $p$ eigenvalues (assuming $n > m$) with orthogonal eigenvectors. It turns out that the principal component vectors are the eigenvectors corresponding to the eigenvalues of $\\Sigma$ in decreasing order.\n",
    "\n",
    "##### Scaling data\n",
    "\n",
    "We typically need to scale our data prior to fitting the PCA model. This is because the variance of a large scale feature is inherently larger than the variance of a small scale feature. So if we have data with vastly differing scales, we will not be recovering the \"hidden structure\" of the data, but rather showing what columns have the largest scale. A common scaling approach is to run the data through `StandardScaler` first.\n",
    "\n",
    "Note that we did not do this here, because our data were constructed to have roughly the same scale.\n",
    "\n",
    "##### In `sklearn`\n",
    "\n",
    "In `sklearn`'s `PCA` these $w$ vectors are stored in `PCA().components_`.\n",
    "\n",
    "We will now use `components_` to more explicitly describe what is going on with PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2221cd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "## demonstrate .components here\n",
    "pca.components_\n",
    "\n",
    "## define w1 and w2\n",
    "w1 = pca.components_[0]\n",
    "w2 = pca.components_[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c472fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(X[:, 0], X[:, 1], alpha=.1)\n",
    "\n",
    "plt.scatter(X[13, 0], X[13, 1], marker='x', color='r', s=100)\n",
    "\n",
    "# The scalar projection is (X* dot w1 times w1, X* dot w1 times w1)\n",
    "plt.plot([X[13,0], X[13,:].dot(w1)*w1[0]], [X[13,1], X[13,:].dot(w1)*w1[1]],'r--')\n",
    "plt.plot([X[13,0], X[13,:].dot(w2)*w2[0]], [X[13,1], X[13,:].dot(w2)*w2[1]],'r--')\n",
    "\n",
    "for length, vector, name in zip(pca.explained_variance_, pca.components_, [\"$w_1$\",\"$w_2$\"]):\n",
    "    v = vector * 3 * np.sqrt(length)\n",
    "    draw_vector(pca.mean_, pca.mean_ + v)\n",
    "    plt.text(v[0],v[1],name, fontsize=16)\n",
    "    \n",
    "    \n",
    "plt.xlabel(\"$x_1$\", fontsize=12)\n",
    "plt.ylabel(\"$x_2$\", fontsize=12)\n",
    "\n",
    "plt.title(\"Original Data\", fontsize=14)\n",
    "\n",
    "\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6f6778",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1,figsize=(8,6))\n",
    "\n",
    "plt.scatter(fit[:, 0], fit[:, 1], alpha=.1)\n",
    "plt.scatter(fit[13, 0], fit[13, 1], alpha=1, c='b', label=\"sklearn PCA\")\n",
    "\n",
    "## calculating the PCA projection by hand\n",
    "plt.scatter(X[13,:].dot(w1), X[13,:].dot(w2), c='r', marker='x', s=100, label=\"By Hand Calulation\")\n",
    "\n",
    "arrowprops=dict(arrowstyle='->',\n",
    "                    linewidth=2,\n",
    "                    shrinkA=0, \n",
    "                    shrinkB=0,\n",
    "                    color=\"black\")\n",
    "ax.annotate('', [3 * np.sqrt(pca.explained_variance_[0]),0], [0,0], arrowprops=arrowprops)\n",
    "plt.text(3 * np.sqrt(pca.explained_variance_[0]),0,\"$w_1$\", fontsize=12)\n",
    "ax.annotate('', [0, 3 * np.sqrt(pca.explained_variance_[1])], [0,0], arrowprops=arrowprops)\n",
    "plt.text(0,3 * np.sqrt(pca.explained_variance_[1]),\"$w_2$\", fontsize=12)\n",
    "plt.plot([X[13,:].dot(w1),X[13,:].dot(w1)], [0, X[13,:].dot(w2)],'r--')\n",
    "plt.plot([0,X[13,:].dot(w1)], [X[13,:].dot(w2), X[13,:].dot(w2)],'r--')\n",
    "\n",
    "plt.legend(fontsize=12)\n",
    "\n",
    "plt.title(\"PCA Projected Data\", fontsize=14)\n",
    "plt.axis(\"equal\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9159ef21",
   "metadata": {},
   "source": [
    "## Explained variance\n",
    "\n",
    "For each weight vector, $w$, we call $\\text{Var}( X w)$ the explained variance due to the principal component $w$. We can think of this as the variance of $X$ explained by the prinicpal component $w$. In `sklearn` we can access this with `explained_variance_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652b7aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## demonstrate explained_variance_\n",
    "pca.explained_variance_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1c0a36",
   "metadata": {},
   "source": [
    "At times it can be useful to think of this in terms of the portion of $\\text{Var}(X)$ explained by the principal direction, $w$. \n",
    "We can access this with `explained_variance_ratio_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a153080c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## demonstrate explained_variance_ratio_\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a469cd1",
   "metadata": {},
   "source": [
    "### When to use PCA\n",
    "\n",
    "1. PCA is useful for **visualizing** your data.\n",
    "\n",
    "When your data lives in a very high dimensional space, projecting it down to the first two principle components can be useful for visualization purposes.\n",
    "\n",
    "Here we will see that projecting this 4 dimensional iris data down to the first 2 PCA dimensions nicely distinguishes the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e440e5b8",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30477a7",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "X, y = load_iris(return_X_y=True, as_frame=True)\n",
    "X.sample(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b7c6ae",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "y.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd5463c",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "pipe = Pipeline([(\"scale\", StandardScaler()), (\"pca\", PCA(n_components=2))])\n",
    "\n",
    "pipe.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9efea66",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = {0:'r', 1:'g', 2:'b'}\n",
    "plt.scatter(pipe.transform(X)[:,0], pipe.transform(X)[:,1], c = y.map(colors))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3185cd4",
   "metadata": {},
   "source": [
    "2. PCA can be used as a preprocessing step before training any machine learning algorithm if you have reason to believe that the \"intrinsic\" dimension of your input space is lower than your number of features.  One example is image data:  since the value of nearby pixels in an image tend to be highly correlated, images tend to live in a much lower dimensional subspace than the number of pixels.\n",
    "\n",
    "Be careful when doing this!  PCA does not look at the $y$ values at all:  it only sees the matrix of features $X$.  It is **entirely possible** that all of the relevant information relating $X$ to $y$ exists in the direction of some small principle components.\n",
    "\n",
    "Also, as we learned in math hour, ridge regression can be thought of as a \"smooth\" version of PCA followed by linear regression.  It also (anecdotally) often performs better.  This makes a bit of sense because we can tune our hyperparameter for ridge regression continuously, but PCA forces us to truncate at an integer number of components.\n",
    "\n",
    "Let's see this in the context of a new dataset.  This data is a cleaned and simplified version of the data from [this study](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0201275)\n",
    "\n",
    "* 'GS_largest_R':  Grip strength of right hand in kilograms\n",
    "* 'GS_largest_L':  Grip strength of left hand in kilograms\n",
    "* 'weight_kg': Subjects weight in kilograms\n",
    "* 'dominant':  1 means right-handed, 0 means left-handed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b25423",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../data/grip_data.csv', usecols= ['GS_largest_R', 'GS_largest_L', 'dominant', 'weight_kg'])\n",
    "df.sample(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb63007",
   "metadata": {},
   "source": [
    "Two tasks we might be interested in are \n",
    "* regressing weight on right and left grip strength measurements\n",
    "* classifying handedness using the same two features.\n",
    "\n",
    "Let's look at the two features colored by handedness:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc908d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "left_hand = (df['dominant'] == 0)\n",
    "right_hand = (df['dominant'] == 1)\n",
    "plt.scatter(df['GS_largest_L'][right_hand], df['GS_largest_R'][right_hand], c = 'b', alpha = 0.7, label = 'right handed')\n",
    "plt.scatter(df['GS_largest_L'][left_hand], df['GS_largest_R'][left_hand], c = 'r', alpha = 0.7, label = 'left handed')\n",
    "\n",
    "plt.xlabel(\"Left strength (kg)\", fontsize=12)\n",
    "plt.ylabel(\"Right strength (kg)\", fontsize=12)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fe6be0",
   "metadata": {},
   "source": [
    "It makes sense that these two features are highly correlated!\n",
    "\n",
    "You might get a good predictive model for weight by regressing on the first principle component (which should be close to the mean of the left and right handed strength for this example).\n",
    "\n",
    "However, a classification algorithm which regressed on the first principle component would do horribly!  In this case, the classes seem to be distinguished by their second principle component, not the first.  This also makes intuitive sense, since in this case it looks like the second principle component would just record the difference between left and right handed grip strength measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39899fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = StandardScaler()\n",
    "pca.fit(scale.fit_transform(df[['GS_largest_L', 'GS_largest_R']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78342ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.components_, pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a519d5ba",
   "metadata": {},
   "source": [
    "## References \n",
    "\n",
    "<a href = \"https://www.tandfonline.com/doi/abs/10.1080/14786440109462720\">On lines and planes of closest fit to systems of points in space</a>\n",
    "\n",
    "<a href=\"https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf\">Univeristy of Waterloo Matrix Cookbook</a>\n",
    "\n",
    "<a href=\"http://www.math.kent.edu/~reichel/courses/monte.carlo/alt4.7d.pdf\">Kent State University Notes on Random Vectors and Matrices</a>\n",
    "\n",
    "<a href=\"http://www.stat.columbia.edu/~fwood/Teaching/w4315/Fall2009/pca.pdf\">Columbia PCA notes</a>\n",
    "\n",
    "<a href=\"https://www.stat.cmu.edu/~cshalizi/uADA/12/lectures/ch18.pdf\">Central Michigan PCA notes</a>\n",
    "\n",
    "[Amoeba's PCA explanation](https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues/140579#140579)\n",
    "\n",
    "[Muscular grip strength normative values for a Korean population from the Korea National Health and Nutrition Examination Survey, 2014–2015.](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0201275)  by Miji Kim, Chang Won Won, Maengkyu Kim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366c22b2",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "\n",
    "This notebook was written for the Erd&#337;s Institute C&#337;de Data Science Boot Camp by Matthew Osborne, Ph. D., 2023.\n",
    "\n",
    "Any potential redistributors must seek and receive permission from Matthew Tyler Osborne, Ph.D. prior to redistribution. Redistribution of the material contained in this repository is conditional on acknowledgement of Matthew Tyler Osborne, Ph.D.'s original authorship and sponsorship of the Erdős Institute as subject to the license (see License.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
