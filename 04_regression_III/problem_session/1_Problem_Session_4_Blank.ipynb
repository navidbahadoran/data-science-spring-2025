{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c55fd5a6",
   "metadata": {},
   "source": [
    "# Problem Session 4\n",
    "\n",
    "The problems in this notebook will cover the content covered in our Regression lectures including:\n",
    "- Regularization\n",
    "- Principle Component Analysis\n",
    "- Categorical Variables and Interactions\n",
    "- Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "36624b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## We first load in packages we will need\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbeaf12",
   "metadata": {},
   "source": [
    "#### 1. Practice creating mock data and fitting models to it\n",
    "\n",
    "Creating your own fake data and fitting models to that data is a good way to practice.  It is nice because you have access to the \"ground truth\" when you make your own data.\n",
    "\n",
    "Another more practical usage of simulation is parametric bootstrapping, which we will cover in a few lectures.\n",
    "\n",
    "It is also *very common* to need to mock up some data during an interview.\n",
    "##### a."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff62420",
   "metadata": {},
   "source": [
    "Use `np.linspace` to create an array of `100` equally spaced values between $0$ and $5$.  Store this in a variable called `x`.\n",
    "\n",
    "Simulate $$y = x + x^2 + \\epsilon \\textrm{ where }\\epsilon \\sim \\mathcal{N}(\\mu = 0,\\sigma = 10)$$ using `np.random.randn`.  Store this in a variable called `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1b65477c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "beffe7dd",
   "metadata": {},
   "source": [
    "##### b.\n",
    "\n",
    "Make a scatterplot of $y$ against $x$ using `plt`.  Also plot the \"ground truth\" relationship. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6009bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "af9c54f5",
   "metadata": {},
   "source": [
    "##### c.\n",
    "\n",
    "Fit three different models to this data:\n",
    "\n",
    "1. The OLS fit of a degree 10 polynomial.\n",
    "2. The ridge regression fit of a degree 10 polynomial using the default `alpha = 1`.  Be sure to use StandardScaler as the first step in the pipeline!\n",
    "3. A pipeline using PCA:  scale -> polynomial transform of degree 10 -> PCA with 5 components -> OLS linear regression.\n",
    "\n",
    "In each case we are fitting a degree 10 polynomial which should **over-fit**.  Our hope is that regularization using either Ridge or PCA will tame that a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5d4792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First import whatever you need\n",
    "\n",
    "# Then instantiate the pipelines\n",
    "pipe_ols = \n",
    "pipe_ridge = \n",
    "pipe_pca = \n",
    "\n",
    "# Finally fit the pipelines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae57534",
   "metadata": {},
   "source": [
    "##### d.\n",
    "\n",
    "Graph all 3 fit models together with the ground truth.  Which is the best approximation of the ground truth?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61da2583",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9abf1d13",
   "metadata": {},
   "source": [
    "You may want to rerun all cells above this one a few times to see the variation in fitted models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "76899e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Click \"Execute above cells\" a few times to see the variation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe826866",
   "metadata": {},
   "source": [
    "Even if you only managed to get through this section it is a profitable use of a problem session!  Depending on the speed of your group you may be able to also tackle the next section.  If not, treat it as homework!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96926347",
   "metadata": {},
   "source": [
    "#### 2. The diamonds dataset\n",
    "\n",
    "We introduce a new \"classic\" dataset.  Our task is to predict the price of diamonds.\n",
    "\n",
    "* price: Price in US dollars.\n",
    "* carat: Weight of the diamond.\n",
    "* cut: Cut quality (ordered worst to best).\n",
    "* color: Color of the diamond (ordered best to worst).\n",
    "* clarity: Clarity of the diamond (ordered worst to best).\n",
    "* x: Length in mm.\n",
    "* y: Width in mm.\n",
    "* z: Depth in mm.\n",
    "* depth: Total depth percentage: 100 * z / mean(x, y)\n",
    "* table: Width of the top of the diamond relative to the widest point.\n",
    "\n",
    "Homepage: https://ggplot2.tidyverse.org/reference/diamonds.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1d85f0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../data/diamonds.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f63cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab49e703",
   "metadata": {},
   "source": [
    "For sake of time we will restrict ourselves to just one categorical feature (`cut`) and one continuous feature (`carat`) in our modeling.  This is only being done for pedagogical purposes!  In a real situation you would want to carefully explore all of the data you have available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fd649b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['cut', 'carat', 'price']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b0f471",
   "metadata": {},
   "source": [
    "#### a."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfc740a",
   "metadata": {},
   "source": [
    "Make a train/test split with 20% of data held aside as the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0f1b037f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ff756d66",
   "metadata": {},
   "source": [
    "##### b. \n",
    "\n",
    "What are the percentage of samples belonging to each level of the `cut` feature?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330a35bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "22819624",
   "metadata": {},
   "source": [
    "##### c. \n",
    "\n",
    "Look at the distribution of price at each level of the `cut` feature.  Do you notice anything strange or unexpected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da57154",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6179da07",
   "metadata": {},
   "source": [
    "##### d. \n",
    "\n",
    "One thing which might be a bit confusing is that the cut quality does not seem to be a very good indicator of price.  Why might that be?\n",
    "\n",
    "Sometimes this happens when two predictors which each have a positive **causal** impact on the outcome are negatively correlated with each other.  In other words, it might be that **all else being equal** a higher quality cut will increase the price, and a larger carat will increase the price, but higher quality cuts are negatively correlated with the size in carats."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade84074",
   "metadata": {},
   "source": [
    "Use the `groupby` and `describe` methods to look at some summary statistics of carat size sorted by cut quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb29ecc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cfbc8392",
   "metadata": {},
   "source": [
    "You should see that the \"Fair\" quality also has the largest mean carat size, while \"Ideal\" quality has the smallest. I am not a domain expert, but this could be due to jewelers needing to cut away more of the original stone to produce better cuts?  This would be something to consult with a jeweler on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4075b73a",
   "metadata": {},
   "source": [
    "##### e.\n",
    "\n",
    "Graph price against carat with color coded by cut quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bd43ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c5766cf9",
   "metadata": {},
   "source": [
    "##### f.\n",
    "\n",
    "The relationship you obtained above does not look linear.  Graph the log of the price against the log of the carat size.  This should look substantially more linear!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55e5bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['log_carat'] = \n",
    "df_train['log_price'] = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c60784f",
   "metadata": {},
   "source": [
    "##### g.\n",
    "\n",
    "We do not have the ability to **experimentally** adjust `cut` and `carat` independently to see the impact on price, but we can still use **statistical control**.\n",
    "\n",
    "We will run a linear regression of `log_price` against `cut` and `log_carat`.  Do better cuts contribute to higher prices when controlling for carat?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7d6daaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8709b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discuss what you think preprocessor does with your team.  Can you test that it does what you think it should?\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", OneHotEncoder(), ['cut']),\n",
    "        ('identity', FunctionTransformer(func = None), ['log_carat'])\n",
    "        ])\n",
    "\n",
    "# Write a pipeline which first uses preprocessor and then uses LinearRegression(fit_intercept = False). \n",
    "# Why do I not want to fit the intercept term?\n",
    "model = \n",
    "\n",
    "# Fit it on the training set.\n",
    "\n",
    "\n",
    "# It is a bit difficult to access the feature names of one part of a pipeline, so I have done it for you.\n",
    "feature_names = model['preprocess'].transformers_[0][1].get_feature_names_out()\n",
    "\n",
    "cut_adjustments = {feature_name: float(model['linear'].coef_[i]) for i,feature_name in enumerate(feature_names)}\n",
    "\n",
    "cut_adjustments_sorted = dict(sorted(cut_adjustments.items(), key=lambda item: item[1]))\n",
    "\n",
    "cut_adjustments_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9afbbf",
   "metadata": {},
   "source": [
    "#### h. Evaluating residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf69127",
   "metadata": {},
   "source": [
    "Make a plot of residuals against predicted values.  Discuss the implications for your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e61d08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6147583f",
   "metadata": {},
   "source": [
    "#### i. Quantifying model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91b9d57",
   "metadata": {},
   "source": [
    "Let's use [mean absolute percentage error](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_percentage_error.html) and [mean absolute error](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_error.html) as our performance metrics.  How does our model perform on the training set?\n",
    "\n",
    "Note:  We have not done any cross validation to compare model performance in this problem session because we have only considered one model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "85751f6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
