{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "839dfb7b",
   "metadata": {},
   "source": [
    "# Problem Session 10\n",
    "## Concrete Compressive Strength I:  Ensemble Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "042b4fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d37682",
   "metadata": {},
   "source": [
    "##### 1. \n",
    "\n",
    "We will work with the following dataset:\n",
    "\n",
    "Yeh,I-Cheng. (2007). Concrete Compressive Strength. UCI Machine Learning Repository. https://doi.org/10.24432/C5PK67.\n",
    "\n",
    "1. Print the ReadMe file and read the variable descriptions.  The file is `Concrete_readme.txt` in this directory.\n",
    "2. Load the data as a pandas DataFrame. The data is located in `../../data/concrete.csv`.\n",
    "    * Note:  the last column `Concrete compressive strength(MPa, megapascals)` is our target variable and the rest are features.\n",
    "3. Make a train/test split.\n",
    "4. Use `sns.pairplot` to visualize the relationship between each feature and the target.\n",
    "    * Discussion question:  which of the following should you use for this visualization and why?\n",
    "        * The full dataset\n",
    "        * The training set\n",
    "        * The testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdddd85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9de0f4ca",
   "metadata": {},
   "source": [
    "#### 2a."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99101f61",
   "metadata": {},
   "source": [
    "In this problem we are removing the usual scaffolding that we supply in the problem sessions!\n",
    "\n",
    "Write your own the code below to find the values of `max_depth` and `n_estimators` for a either a Random Forest Regressor or an Extra Trees Regressor with the lowest average cross-validation RMSE.\n",
    "\n",
    "Your code should accomplish the following:\n",
    "1. Make a stratified 5-fold split of the training data.\n",
    "2. Select `max_depth` from `range(1,11)`  and `n_estimators` from the two choices `[100,500]` to minimize cross validation RMSE.\n",
    "\n",
    "Try not to copy/paste from earlier problem sessions:  talk through the logic and write your own cross-validation loop.\n",
    "\n",
    "Some further questions to consider when training a random forest regressors:\n",
    "\n",
    "* Is scaling necessary as a preprocessing step?  Why or why not?\n",
    "* Does colinearity of features matter?\n",
    "* Should you do feature selection first?\n",
    "* What bias/variance impact do you think increasing `n_estimators` or `max_depth` will have on the model?\n",
    "* How might you decide whether to use Random Forest or Extra Trees?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e29311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d7db2f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A = \n",
      "[[[70 26 68]\n",
      "  [14 79 94]\n",
      "  [31 26 96]\n",
      "  [45 45 23]]\n",
      "\n",
      " [[75 41 85]\n",
      "  [54 90 39]\n",
      "  [90 70 17]\n",
      "  [39 23 65]]] \n",
      " \n",
      " The minimum value of A is A[(np.int64(0), np.int64(1), np.int64(0))] = 14\n"
     ]
    }
   ],
   "source": [
    "# Hint:  at some point you will want to know both the minimum cross validation RMSE and the parameter values used to obtain that\n",
    "# If you want the index of the maximal element of a numpy array `A`, the following code will give you the index\n",
    "\n",
    "# Maxing a toy array first\n",
    "np.random.seed(215)\n",
    "A = np.random.randint(100, size = (2,4,3))\n",
    "\n",
    "# Here is how to find the index of the maximal value\n",
    "min_index = np.unravel_index(np.argmin(A), A.shape)\n",
    "\n",
    "print(f\"A = \\n{A} \\n \\n The minimum value of A is A[{min_index}] = {A[min_index]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf93e31e",
   "metadata": {},
   "source": [
    "##### b.\n",
    "\n",
    "In this problem you will learn about `GridSearchCV`, <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\">https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html</a>, a handy class from `sklearn` that makes hyperparameter tuning through a grid search and cross-validation quicker to code up than writing out a series of `for` loops.\n",
    "\n",
    "\n",
    "Read through the code chunks below and fill in the missing code to run the same grid search cross-validation you did above with `GridSearchCV`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a150f896",
   "metadata": {},
   "outputs": [],
   "source": [
    "## first import GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e82c25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This will also take 1-2 minutes to run\n",
    "grid_cv = GridSearchCV(, # first put the model object here\n",
    "                          param_grid = {'max_depth':, # place the grid values for max_depth and\n",
    "                                        'n_estimators':}, # and n_estimators here\n",
    "                          scoring = , # put the score we are trying to optimize here as a string, \"‘neg_root_mean_squared_error’\"\n",
    "                                      # Note that \"score\" is the opposite of \"loss\":  bigger score is better.\n",
    "                          cv = ) # put the number of cv splits here\n",
    "\n",
    "## you fit it just like a model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256cff2a",
   "metadata": {},
   "source": [
    "Once a `GridSearchCV` is fit you are easily able to find what hyperparameter combinations were best, what the optimal score was as well as get access to the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917bc725",
   "metadata": {},
   "outputs": [],
   "source": [
    "## You can find the hyperparameter grid point that\n",
    "## gave the best performance like so\n",
    "## .best_params_\n",
    "grid_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe674ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "## You can find the best score like so\n",
    "## .best_score_\n",
    "grid_cv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e56a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calling best_estimator_ returns the model with the \n",
    "## best avg cv performance after it has been refit on the\n",
    "## entire data set\n",
    "grid_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998b3261",
   "metadata": {},
   "source": [
    "The `best_estimator_` is a model with the optimal hyperparameters that has been fit on the entire training set. Try and predict the compressive strength on the training set with the `best_estimator_` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eee5234",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b19398",
   "metadata": {},
   "source": [
    "If you want to look at all of the results, you can do that as well with `.cv_results`. Try that below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741942d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## You can get all of the results with cv_results_\n",
    "grid_cv.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb88d266",
   "metadata": {},
   "source": [
    "##### c.\n",
    "\n",
    "Using either the `best_estimator_` fitted model or a refitted model according to your results from the `for` loop cross-validation find the feature importance scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe4005c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32117e18",
   "metadata": {},
   "source": [
    "#### 5. Bagging one of the previous models\n",
    "\n",
    "Consider the following regression algorithms:\n",
    "\n",
    "* kNN with $k=2$\n",
    "* kNN with $k=100$\n",
    "* Linear Regression\n",
    "* Support Vector Regressor using RBF kernel and $\\gamma = 0.1$\n",
    "* Support Vector Regressor using RBF kernel and $\\gamma = 10$\n",
    "\n",
    "Of these which are likely to benefit from bagging, if any?\n",
    "\n",
    "Choose one algorithm which you think could be improved by bagging and compare cross-validation RMSE of the base model and the bagged model.  Did bagging improve performance?  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e919512",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7652db58",
   "metadata": {},
   "source": [
    "#### 3.  Write your own Bagging Regressor class  \n",
    "\n",
    "Write your own BaggingRegressor class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2974aee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class CustomBaggingRegressor:\n",
    "    '''\n",
    "    Trains a sequence of regressors on bootstrap resamples of the training data.\n",
    "    Prediction is performed by taking the mean of the predictions of all regressors.\n",
    "    This is only designed to work with MSE loss.\n",
    "    '''\n",
    "    def __init__(self, base_estimator, num_estimators=10, kwargs={}):\n",
    "        '''\n",
    "        Parameters:\n",
    "            base_estimator: A regression class from sklearn\n",
    "            num_estimators: The number of estimators in the ensemble.\n",
    "            kwargs: A dictionary of keyword arguments to pass to the estimators.\n",
    "        \n",
    "        Attributes:\n",
    "            self.estimators: A list of instantiated base estimators.\n",
    "        '''\n",
    "        self.kwargs = \n",
    "        self.num_estimators = \n",
    "        self.estimators = \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "        Parameters:\n",
    "            X: numpy array of shape (n, p), where n is the number of samples.\n",
    "            y: numpy array of shape (n,), target values.\n",
    "        '''\n",
    "        rng = np.random.default_rng()\n",
    "        n_samples = X.shape[0]\n",
    "        for estimator in self.estimators:\n",
    "            # Generate bootstrap indices (sampling rows, axis=0)\n",
    "            indices =  # use rng.choice\n",
    "            X_boot = \n",
    "            y_boot = \n",
    "            estimator.fit(X_boot, y_boot)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Predict using the ensemble by averaging predictions from all estimators.\n",
    "\n",
    "        Parameters:\n",
    "            X: numpy array of shape (n, p)\n",
    "        \n",
    "        Returns:\n",
    "            preds: numpy array of shape (n,), the aggregated predictions.\n",
    "        '''\n",
    "        # Collect predictions from each estimator and take their mean\n",
    "        preds = \n",
    "        return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c259cabc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14264603928267197"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make sure that your class is able to run the following code.\n",
    "# Does increasing the number of estimators decrease the MSE?\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X = np.random.normal(size = (100,2))\n",
    "y = X[:,0]**2 + X[:,0]*X[:,1]\n",
    "\n",
    "model =  CustomBaggingRegressor(\n",
    "            base_estimator = DecisionTreeRegressor, \n",
    "            num_estimators = 1, # try 1 and 10 a bunch of times.\n",
    "            kwargs = {'max_depth' : 10}\n",
    "            )\n",
    "\n",
    "model.fit(X,y)\n",
    "\n",
    "mean_squared_error(y, model.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51c5eb3",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "\n",
    "This notebook was written for the Erd&#337;s Institute Data Science Boot Camp by Steven Gubkin, 2025."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erdos_spring_2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
