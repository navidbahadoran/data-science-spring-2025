{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b382d20a",
   "metadata": {},
   "source": [
    "# Problem Session 9\n",
    "## Classifying Pumpkin Seeds II\n",
    "\n",
    "In this notebook you continue to work with the pumpkin seed data from <a href=\"https://link.springer.com/article/10.1007/s10722-021-01226-0\">The use of machine learning methods in classification of pumpkin seeds (Cucurbita pepo L.)</a> by Koklu, Sarigil and Ozbek (2021)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b336fe05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b9f02d",
   "metadata": {},
   "source": [
    "#### 1. Load then prepare the data\n",
    "\n",
    "\n",
    "- Load the data stored in `Pumpkin_Seeds_Dataset.xlsx` in the `Data` folder,\n",
    "- Create a column `y` where `y=1` if `Class=Ürgüp Sivrisi` and `y=0` if `Class=Çerçevelik` and\n",
    "- Make a train test split setting $10\\%$ of the data aside as a test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f01868",
   "metadata": {},
   "source": [
    "##### Sample Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36da466e",
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = pd.read_excel(\"../../Data/Pumpkin_Seeds_Dataset.xlsx\")\n",
    "\n",
    "seeds['y'] = 0\n",
    "\n",
    "seeds.loc[seeds.Class=='Ürgüp Sivrisi', 'y']=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8fd445",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2920249",
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds_train, seeds_test = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bee9ccf",
   "metadata": {},
   "source": [
    "#### 2. Refresh your memory\n",
    "\n",
    "If you need to refresh your memory on these data and the problem, you may want to look at a small subset of the data, look back on `Problem Session 8` and/or browse Figure 5 and Table 1 of this paper, <a href=\"pumpkin_seed_paper.pdf\">pumpkin_seed_paper.pdf</a>\n",
    "\n",
    "We will store our different model accuracies in a dictionary for easy comparson at the end of the notebook.  I am starting it off with the two best models from problem session 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854f7f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_accs = {\n",
    "         \"knn\" : 0.886,\n",
    "         \"log_reg\": 0.867\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a26c4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds_train.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e8a15e",
   "metadata": {},
   "source": [
    "#### 3. Principal components analysis (PCA)\n",
    "\n",
    "One way you may use PCA is as a data preprocessing step for supervised learning tasks. In this problem you will try it as a preprocessing step for the pumpkin seed data and see if this preprocessing step helps your model outperform the models from `Problem Session 8`.\n",
    "\n",
    "##### a. \n",
    "\n",
    "Run the training data through PCA with two components and then plot the resulting principal values. Color each point by its class.\n",
    "\n",
    "<i>Hint: Remember to scale the data before running it through PCA</i>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11842e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import \n",
    "from sklearn.preprocessing import\n",
    "from sklearn.pipeline import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ac1fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = seeds_train.columns[:-2]\n",
    "\n",
    "pca = # make scale/pca pipe\n",
    "\n",
    "# fit model object\n",
    "\n",
    "pca_values = # transformed values here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950afa62",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,5))\n",
    "\n",
    "plt.scatter(pca_values[seeds_train.y==0, 0], \n",
    "            pca_values[seeds_train.y==0, 1],\n",
    "            color = 'b',\n",
    "            label=\"$y=0$\",\n",
    "            alpha=.6)\n",
    "\n",
    "plt.scatter(pca_values[seeds_train.y==1, 0], \n",
    "            pca_values[seeds_train.y==1, 1],\n",
    "            color='r',\n",
    "            marker='x',\n",
    "            label=\"$y=1$\",\n",
    "            alpha=.6)\n",
    "\n",
    "plt.legend(fontsize=10)\n",
    "\n",
    "plt.xlabel(\"First PCA Value\", fontsize=12)\n",
    "plt.ylabel(\"Second PCA Value\", fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158c4c8b",
   "metadata": {},
   "source": [
    "##### b.\n",
    "\n",
    "How does the PCA with only two componenets appear to separate the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91b2c3e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f2d8fabf",
   "metadata": {},
   "source": [
    "##### c.\n",
    "\n",
    "Run 5-fold cross-validation below to find the optimal value of $k$ for a $k$ nearest neighbors model fit on the first and second PCA values. What is the optimal $k$ and the associated average cross-validation accuracy? How does this compare to the accuracies from `Problem Session 8`?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8141fd09",
   "metadata": {},
   "source": [
    "##### Sample Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e774de9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import \n",
    "from sklearn.model_selection import \n",
    "from sklearn.metrics import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c88813c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 5\n",
    "kfold = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f63f08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors = range(1, 51)\n",
    "\n",
    "pca_2_accs = np.zeros((n_splits, len(neighbors)))\n",
    "\n",
    "# Note:  switching to using \"enumerate\" from this point in the bootcamp forward.\n",
    "for i,(train_index, test_index) in enumerate(kfold.split(seeds_train, seeds_train.y)):\n",
    "    print(\"CV Split\", i)\n",
    "    seeds_tt = \n",
    "    seeds_ho = \n",
    "    \n",
    "    ## Note, putting the PCA here speeds up the loop\n",
    "    pca_pipe = \n",
    "    \n",
    "    pca_tt = \n",
    "    pca_ho = \n",
    "    \n",
    "    for j, n_neighbors in enumerate(neighbors):\n",
    "        # No need to scale knn first since PCA is handling that\n",
    "        knn =\n",
    "        \n",
    "        knn.fit()\n",
    "        \n",
    "        pred = knn.predict()\n",
    "        \n",
    "        pca_2_accs[i,j] = accuracy_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502715b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,5))\n",
    "\n",
    "plt.plot(neighbors, \n",
    "         np.mean(pca_2_accs, axis=0),\n",
    "         '-o')\n",
    "\n",
    "plt.xticks(fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "plt.xlabel(\"$k$\", fontsize=12)\n",
    "plt.ylabel(\"Avg. CV Accuracy\", fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2161e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The best mean CV accuracy of {np.max(np.mean(pca_2_accs, axis=0)):.3f} was achieved with k = {neighbors[np.argmax(np.mean(pca_2_accs, axis=0))]}' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e666c6a",
   "metadata": {},
   "source": [
    "##### d.\n",
    "\n",
    "We can think of the number of components used in PCA as another hyperparameter we can tune.\n",
    "\n",
    "Fill in the missing code below to find the optimal number of components and $k$ pairing for this problem. What is the best average cross-validation accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68400ce",
   "metadata": {},
   "source": [
    "##### Sample Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a6b186",
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors = range(1, 51)\n",
    "comps = range(2,6)\n",
    "\n",
    "pca_accs = np.zeros((n_splits, len(comps), len(neighbors)))\n",
    "\n",
    "for i,(train_index, test_index) in enumerate():\n",
    "    print(\"CV Split\", i)\n",
    "    seeds_tt = \n",
    "    seeds_ho =\n",
    "    \n",
    "    for j, n_comps in enumerate(comps):\n",
    "        pca_pipe = \n",
    "        pca.pipe.fit()\n",
    "\n",
    "        pca_tt = \n",
    "        pca_ho = \n",
    "        \n",
    "        for k, n_neighbors in enumerate(neighbors):\n",
    "            knn = \n",
    "            \n",
    "            knn.fit()\n",
    "\n",
    "            pred = knn.predict()\n",
    "\n",
    "            pca_accs[i,j,k] = accuracy_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d139d565",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_index = np.unravel_index(np.argmax(np.mean(pca_accs, axis=0), axis=None), \n",
    "                                       np.mean(pca_accs, axis=0).shape)\n",
    "\n",
    "\n",
    "print(f\"The pair with the highest AVG CV Accuracy was k = {neighbors[max_index[1]]} and number of components = {comps[max_index[0]]:.1f}\")\n",
    "print(f\"The highest AVG CV Accuracy was {np.max(np.mean(pca_accs, axis=0)):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bce104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this best model to our dict of model accuracies\n",
    "model_accs['pca_knn'] = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09293b3",
   "metadata": {},
   "source": [
    "#### 4. Trying Bayes based classifiers\n",
    "\n",
    "Build LDA, QDA and naive Bayes' models on these data by filling in the missing code for the cross-validation below. \n",
    "\n",
    "Do these outperform your PCA-$k$-NN model from above?="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f2d5bd",
   "metadata": {},
   "source": [
    "##### Sample Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60674ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import \n",
    "from sklearn.discriminant_analysis import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba1b8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes_accs = np.zeros((n_splits, 3))\n",
    "\n",
    "for i, (train_index, test_index) in enumerate():\n",
    "    seeds_tt = seeds_train.iloc[train_index]\n",
    "    seeds_ho = seeds_train.iloc[test_index]\n",
    "    \n",
    "    ## Linear Discriminant Analysis\n",
    "    lda = \n",
    "    \n",
    "    lda.fit()\n",
    "    lda_pred = \n",
    "    \n",
    "    bayes_accs[i, 0] = accuracy_score()\n",
    "    \n",
    "    ## Quadratic Discriminant Analysis\n",
    "    qda = \n",
    "    \n",
    "    qda.fit()\n",
    "    \n",
    "    qda_pred = \n",
    "    \n",
    "    bayes_accs[i, 1] = accuracy_score()\n",
    "    \n",
    "    \n",
    "    ## Gaussian Naive Bayes\n",
    "    nb = \n",
    "    \n",
    "    nb.fit()\n",
    "    \n",
    "    nb_pred = nb.predict()\n",
    "    \n",
    "    bayes_accs[i, 2] = accuracy_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2755d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(bayes_accs, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d586e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Come up with a reasonable short name for your best model and store the accuracy rounded to the nearest thousandth place here.\n",
    "model_accs[''] = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a02c210",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_accs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd560fa",
   "metadata": {},
   "source": [
    "#### 5. A support vector machine classifier\n",
    "\n",
    "In this problem you will work to build a support vector classifier on these data.\n",
    "\n",
    "##### a.\n",
    "\n",
    "Start by importing the support vector classifier from `sklearn`.  We will use the default kernel which is `rbf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e3eb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40137da5",
   "metadata": {},
   "source": [
    "##### b.\n",
    "\n",
    "You will now perform hyperparameter tuning on the `C` parameter of the support vector classifier. Fill in the missing pieces of the code below to perform 5-fold cross-validation for different values of `C`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce4d4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "## set the number of CV folds\n",
    "n_splits = 5\n",
    "\n",
    "## Make the kfold object\n",
    "kfold = StratifiedKFold()\n",
    "\n",
    "## the values of C you will try\n",
    "Cs = [.01, .1, 1, 10, 25, 50, 75, 100, 125, 150]\n",
    "\n",
    "## this will hold the CV accuracies\n",
    "C_accs1 = np.zeros((n_splits, len(Cs)))\n",
    "\n",
    "\n",
    "## the cross-validation\n",
    "for i,(train_index, test_index) in enumerate():\n",
    "    seeds_tt = \n",
    "    seeds_ho = \n",
    "    \n",
    "    for j,C in enumerate(Cs):\n",
    "        pipe = \n",
    "    \n",
    "        pipe.fit()\n",
    "    \n",
    "        pred = \n",
    "\n",
    "        C_accs1[i, j] = accuracy_score()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdae7d4b",
   "metadata": {},
   "source": [
    "##### c.\n",
    "\n",
    "Plot the average cross-validation accuracy against the $\\log$ of `C`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb306690",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8,6))\n",
    "\n",
    "plt.plot(np.log10(np.array(Cs)), \n",
    "         np.mean(C_accs1, axis=0), \n",
    "         '-o')\n",
    "\n",
    "plt.xlabel(\"$\\log(C)$\", fontsize=12)\n",
    "plt.ylabel(\"Avg. CV Accuracy\", fontsize=12)\n",
    "plt.xticks(np.arange(-2,3,.5),fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1def0e3d",
   "metadata": {},
   "source": [
    "##### e.\n",
    "\n",
    "What was the optimal value of `C`, what was the average cross-validation accuracy for this value of `C`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ce2f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_cv_accuracy = np.mean(C_accs1, axis=0)\n",
    "optimal_index = np.argmax(mean_cv_accuracy)\n",
    "optimal_C = Cs[optimal_index]\n",
    "optimal_accuracy = mean_cv_accuracy[optimal_index]\n",
    "\n",
    "print(f\"The optimal C was {optimal_C} which gave a mean CV accuracy of {optimal_accurac:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65a07cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_accs['svc'] = np.round(optimal_accuracy,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e306f5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_accs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd864a4",
   "metadata": {},
   "source": [
    "These models all perform quite similarly!  It is very possible that we just don't have a set of features which are sufficiently discriminating to do any better.  Let's actually find which two training samples with different class are closest to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03af94e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import KDTree\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Scale the feature data\n",
    "scaler = StandardScaler()\n",
    "scaled_X = scaler.fit_transform(seeds_train[features])\n",
    "\n",
    "# Construct a KDTree for fast nearest-neighbor search\n",
    "kdt = KDTree(scaled_X, leaf_size=30, metric='euclidean')\n",
    "\n",
    "# Find the two nearest neighbors for each point\n",
    "distances, indices = kdt.query(scaled_X, k=2, return_distance=True)\n",
    "\n",
    "# Sort indices by nearest neighbor distance (excluding self-distance)\n",
    "sorted_indices = np.argsort(distances[:, 1])  # Only the 2nd column matters\n",
    "\n",
    "# Reorder neighbor pairs accordingly\n",
    "sorted_pairs = indices[sorted_indices]\n",
    "\n",
    "# Identify the first pair with different class labels\n",
    "labels = seeds_train.y.values\n",
    "mismatch_index = np.argmax(labels[sorted_pairs[:, 0]] != labels[sorted_pairs[:, 1]])\n",
    "\n",
    "# Retrieve the mismatched pair\n",
    "index_1, index_2 = sorted_pairs[mismatch_index]\n",
    "\n",
    "# Display the mismatched data points\n",
    "seeds_train.iloc[[index_1, index_2], :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a591d247",
   "metadata": {},
   "source": [
    "We can see that these are extremely close to each other despite having different classes.  If there are many such examples, it may be impossible for us to get a better classification accuracy.\n",
    "\n",
    "It is possible that these two cultivars could be very different genetically, very different morphologically as adult plants, and yet have seeds which are similar enough that some of them cannot be distinguished from each other based exclusively on their geometry.  \n",
    "\n",
    "It is also possible that the seeds **are** distinct, but that some of our samples have been mislabeled.  This would also spell doom for any improved accuracy on this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059eadc8",
   "metadata": {},
   "source": [
    "#### 6. (OPTIONAL) LDA for supervised dimensionality reduction\n",
    "\n",
    "Only do this section if you have time.\n",
    "\n",
    "While we introduced linear discriminant analysis (LDA) as a classification algorithm, it was originally proposed by Fisher as a supervised dimension reduction technique, <a href=\"https://digital.library.adelaide.edu.au/dspace/bitstream/2440/15227/1/138.pdf\">https://digital.library.adelaide.edu.au/dspace/bitstream/2440/15227/1/138.pdf</a>. In particular, the initial goal was to project the features, $X$, corresponding to a binary output, $y$, onto a single dimension which best separates the possible classes. This single dimension has come to been known as <i>Fisher's discriminant</i>.\n",
    "\n",
    "In the case of two classes, we are projecting onto the line connecting the class sample means.  However we are **not** projecting orthogonally with respect to the Euclidean metric!  As discussed in the week 8 math hour, we will end up doing orthogonal projection with respect to the Mahalanobis metric of the learned LDA covariance matrix.\n",
    "\n",
    "Walk through the code below to perform this supervised dimension reduction technique on these data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b17e3c",
   "metadata": {},
   "source": [
    "##### a.\n",
    "\n",
    "First make a validation set from the training set for demonstration purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ed3724",
   "metadata": {},
   "source": [
    "##### Sample Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc74fba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## First we make a validation set for demonstration purposes\n",
    "seed_tt, seeds_val = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d912df47",
   "metadata": {},
   "source": [
    "##### b.\n",
    "\n",
    "Now make a pipeline that first scales the data and ends with linear discriminant analysis. Then fit the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1aa6dc2",
   "metadata": {},
   "source": [
    "##### Sample Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d30ae85",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a76d4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1034609",
   "metadata": {},
   "source": [
    "##### c. \n",
    "\n",
    "Now calculate the Fisher discriminant by using `transform` with the pipeline you fit in <i>b.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e0369b",
   "metadata": {},
   "source": [
    "##### Sample Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73d186b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fish = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f86ffdc",
   "metadata": {},
   "source": [
    "##### d. \n",
    "\n",
    "To visualize how LDA separated the two classes while projecting the 12 dimensional data onto a one dimensional subspace you can plot a histogram of the Fisher discriminant colored by the pumpkin seed class of the observation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d6ec83",
   "metadata": {},
   "source": [
    "##### Sample Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcf457e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "plt.hist(fish[seeds_tt.y==0], \n",
    "         color='blue',\n",
    "         edgecolor=\"black\",\n",
    "         label=\"$y=0$\")\n",
    "\n",
    "plt.hist(fish[seeds_tt.y==1], \n",
    "         color='orange', \n",
    "         hatch='/', \n",
    "         alpha=.6,\n",
    "         edgecolor=\"black\",\n",
    "         label=\"$y=1$\")\n",
    "\n",
    "plt.xlabel(\"Fisher Discriminant\", fontsize=12)\n",
    "plt.ylabel(\"Count\", fontsize=12)\n",
    "plt.legend(fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a9eccb",
   "metadata": {},
   "source": [
    "##### e.\n",
    "\n",
    "While there is some separation between the two classes, it is not perfect, this should be expected based on the exploratory data analysis you did in `Problem Session 8`.\n",
    "\n",
    "We could use this discriminant in order to make classifications, for example by setting a simple cutoff value or as input into a different classification algorithm.\n",
    "\n",
    "However, it is important to note that the LDA algorithm maximizes the separation of the two classes among observations of the training set. It is possible that separation would not be as good for data the algorithm was not trained on.\n",
    "\n",
    "In this example we can visually inspect by plotting a histogram of the Fisher discriminant values for the validation set we created. Does the separation seem as pronounced on the validation data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b4766d",
   "metadata": {},
   "source": [
    "##### Sample Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb1bfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fish_val = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ee620b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "plt.hist(fish_val[seeds_val.y==0], \n",
    "         color='blue',\n",
    "         edgecolor=\"black\",\n",
    "         label=\"$y=0$\")\n",
    "\n",
    "plt.hist(fish_val[seeds_val.y==1], \n",
    "         color='orange', \n",
    "         hatch='/', \n",
    "         alpha=.6,\n",
    "         edgecolor=\"black\",\n",
    "         label=\"$y=1$\")\n",
    "\n",
    "plt.xlabel(\"Fisher Discriminant\", fontsize=12)\n",
    "plt.ylabel(\"Count\", fontsize=12)\n",
    "plt.legend(fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc242bfc",
   "metadata": {},
   "source": [
    "There appears to be a little more overlap, but overall the separation appears similar on the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307b990a",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "\n",
    "This notebook was written for the Erd&#337;s Institute C&#337;de Data Science Boot Camp by Matthew Osborne, Ph. D., 2023. Modified by Steven Gubkin 2024.\n",
    "\n",
    "Any potential redistributors must seek and receive permission from Matthew Tyler Osborne, Ph.D. prior to redistribution. Redistribution of the material contained in this repository is conditional on acknowledgement of Matthew Tyler Osborne, Ph.D.'s original authorship and sponsorship of the Erdős Institute as subject to the license (see License.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
