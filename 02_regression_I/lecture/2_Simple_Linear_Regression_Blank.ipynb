{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52da8da7",
   "metadata": {},
   "source": [
    "# Simple Linear Regression\n",
    "\n",
    "We start our section on regression with the simplest model, simple linear regression.\n",
    "\n",
    "## What we will accomplish in this notebook\n",
    "\n",
    "In this notebook we will:\n",
    "- Introduce the simple linear regression model,\n",
    "- Discuss and visualize its assumptions,\n",
    "- Demonstrate how to fit the model theoretically and practically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3b710adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from seaborn import set_style\n",
    "\n",
    "set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a13f7a5",
   "metadata": {},
   "source": [
    "## The model\n",
    "\n",
    "In simple linear regression (SLR) we have a variable we would like to predict, $y$, and a single feature $x$. The form of $f$ in the supervised learning framework we have discussed is as follows:\n",
    "\n",
    "$$\n",
    "y = f(x) + \\epsilon = \\beta_0 + \\beta_1 x + \\epsilon,\n",
    "$$\n",
    "\n",
    "where $\\beta_0, \\beta_1 \\in \\mathbb{R}$ are constants we must estimate and we assume that $\\epsilon \\sim N(0,\\sigma^2)$ is a normally distributed error term independent of $x$.\n",
    "\n",
    "### Visualizing the model\n",
    "\n",
    "Let's think about what this model is saying about the outcome variable, $y$. For help we will look at the picture drawn below.\n",
    "\n",
    "<img src=\"lecture_assets/slr_curves.png\" width=\"60%\"></img>\n",
    "\n",
    "Above we see both the systematic part and the random error. For a given value of $x$ you can find the theoretically possible values for $y$ by going to the line $\\beta_0 + \\beta_1 x$ and randomly drawing an error term from the normal distribution centered on the line. We can also see one of our key assumptions at play: no matter what the value of $x$, our errors are drawn from the same exact bell curve.\n",
    "\n",
    "You can look at a 3D version of the same diagram [here](https://www.desmos.com/3d/09db6f9c8d).\n",
    "\n",
    "If our assumptions hold, we can derive some nice features about estimates and predictions made in the course of fitting this model that we may touch on in our problem session and/or the homework.\n",
    "\n",
    "### Fitting the model\n",
    "\n",
    "Given $n$ observations of pairs $(x_i,y_i)$, $i = 1,\\dots,n$ how do we fit this model, what do we need to estimate? Remember that our goal is to find an estimate of $f$ called $\\hat{f}$. For SLR this means that we need to estimate $\\beta_0$ and $\\beta_1$, i.e. we need to find $\\hat{\\beta_0}$ and $\\hat{\\beta_1}$.\n",
    "\n",
    "#### Minimizing mean square error (MSE)\n",
    "\n",
    "We find a $\\hat{\\beta_0}$ and $\\hat{\\beta_1}$ by minimizing a <i>loss function</i>, namely the mean square error (MSE), which is given by:\n",
    "\n",
    "$$\n",
    "\\operatorname{MSE}(\\beta) = \\frac{1}{n}\\sum_{i=1}^n (y_i - f_\\beta(x_i))^2.\n",
    "$$\n",
    "\n",
    "For the particular case of SLR this is:\n",
    "\n",
    "$$\n",
    "\\operatorname{MSE}(\\beta_0,\\beta_1) = \\frac{1}{n}\\sum_{i=1}^n (y_i - \\beta_0 - \\beta_1 x_i)^2.\n",
    "$$\n",
    "\n",
    "The MSE represents the average square error of the estimate from the actual value, for a measurement of the average error that is on the same scale as $y$ you can take the square root of the MSE known as the Root MSE or RMSE.\n",
    "\n",
    "You can look at a 2D visualization of the MSE at [this link](https://www.desmos.com/calculator/ewqexkfjm1) and a 3D visualization of the MSE at [this link](https://www.desmos.com/3d/72e4cb5e40).\n",
    "\n",
    "You can do some mathematics to find the values $\\hat{\\beta_0}$ and $\\hat{\\beta_1}$ that minimize the MSE.  This was covered in math hour.  The 5 second summary is that you can either:\n",
    "\n",
    "* Use calculus:  take the gradient of the MSE with respect to the parameters and set it equal to zero.\n",
    "* Use linear algebra:  use dot products to project $\\vec{y}$ onto the subspace spanned by $\\vec{1}$ and $\\vec{x}$\n",
    "\n",
    "Either way you do it we find that\n",
    "\n",
    "$$\n",
    "\\hat{\\beta_0} = \\overline{y} - \\hat{\\beta_1} \\overline{x}, \\text{ and}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{\\beta_1} = \\frac{\\sum_{i=1}^n \\left( x_i - \\overline{x}\\right)\\left( y_i - \\overline{y} \\right)}{\\sum_{i=1}^n \\left(x_i - \\overline{x} \\right)^2} = \\frac{\\text{cov}(x,y)}{\\text{var}(x)},\n",
    "$$\n",
    "\n",
    "where $\\overline{x}$ and $\\overline{y}$ are the means of $x$ and $y$ respectively, $\\text{cov}$ denotes the sample covariance and $\\text{var}$ denotes the sample variance.\n",
    "\n",
    "<i>Note:</i> MSE is used as the default loss function for simple linear regression for a number of reasons stemming from its roots as a statistical regression technique. Importantly, MSE is differentiable with respect to $\\beta_i$ and is a convex function. As seen in math hour we also minimize the MSE when performing a maximum likelihood estimate of the parameters.  However, MSE is not the only loss function people consider in this type of model. Check out the corresponding `Optional Extra Practice` notebook to learn about mean absolute error (MAE)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7898b2e3",
   "metadata": {},
   "source": [
    "## Our first regression problem\n",
    "\n",
    "We will use [the UC Irvine abalone dataset](https://archive.ics.uci.edu/dataset/1/abalone).\n",
    "\n",
    ">Predicting the age of abalone from physical measurements.  The age of abalone is determined by cutting the shell through the cone, staining it, and counting the number of rings through a microscope -- a boring and time-consuming task.  Other measurements, which are easier to obtain, are used to predict the age.\n",
    ">\n",
    ">From the original data examples with missing values were removed (the majority having the predicted value missing), and the ranges of the continuous values have been scaled for use with an ANN (by dividing by 200).\n",
    "\n",
    "For the purposes of this notebook we will focus on using the length as a predictor of the age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d73d6653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We deliberately left the ucimlrepo library out of the environment.\n",
    "# so you can see how to install new libraries to the environment when needed.\n",
    "# Uncomment the following line and run the cell to install it.\n",
    "\n",
    "#!pip install ucimlrepo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ffc345",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "  \n",
    "# fetch dataset \n",
    "abalone = fetch_ucirepo(id=1) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = abalone.data.features \n",
    "y = abalone.data.targets \n",
    "  \n",
    "# metadata \n",
    "print(abalone.metadata) \n",
    "  \n",
    "# variable information \n",
    "print(abalone.variables) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ef6661",
   "metadata": {},
   "source": [
    "From the dataset description, the continuous features have been scaled down by a factor of $200$.  Let's undo that to make our results more interpretable.  Let's also add $1.5$ to our target so that it represents age instead of the number of rings counted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "256d67df",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.loc[:,['Length', 'Diameter', 'Height', 'Whole_weight', 'Shucked_weight', 'Viscera_weight', 'Shell_weight']] *= 200\n",
    "y += 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a71d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X['Length'], y)\n",
    "plt.xlabel('Length (mm)')\n",
    "plt.ylabel('Age (years)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8ec0bd",
   "metadata": {},
   "source": [
    "This plot does show a roughly linear relationship.  It also shows **heteroskedasticity**: the variance of the error appears to be correlated with the predictor!  We will ignore this subtlety for now and just see how to fit the model.  One supposition of our model is **homoskedasticity**:  the error terms all have the same variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12524231",
   "metadata": {},
   "source": [
    "#### Implementing SLR in `sklearn`\n",
    "\n",
    "While we can code up coefficient estimates for SLR using the formulae we just derived, we can also use `sklearn`'s `LinearRegression` model object.\n",
    "\n",
    "Here is the documentation for `LinearRegression`, <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\">https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html</a>.\n",
    "\n",
    "`sklearn` is <b>the</b> python machine learning model package. We will use it frequently throughout these notebooks. `sklearn` models follow a similar pattern we will now demonstrate.\n",
    "\n",
    "##### Import the model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99bba41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## First we import the model class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7d84c7",
   "metadata": {},
   "source": [
    "##### Make a model object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3693681",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now we make an instance of the model\n",
    "## To do this just call the name of the model class, LinearRegression()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfad672",
   "metadata": {},
   "source": [
    "##### `fit`ting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5486b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now we fit the model\n",
    "## this is typically model.fit(X, y)\n",
    "## NOTE! X has to be a 2D array, think matrix or column vector\n",
    "## either use X[['Length']] or X['Length'].values.reshape(-1,1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905dfa85",
   "metadata": {},
   "source": [
    "##### Making `predict`ions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2128c99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## model.predict will tell us what the model says\n",
    "## for an array of input values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d763262",
   "metadata": {},
   "source": [
    "Those are the basic steps for most every `sklearn` model we will work with. However, models typically have features and methods that are unique to them. We will review a few of those for `LinearRegression` below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34249621",
   "metadata": {},
   "source": [
    "##### Simple linear regression content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8178dd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## We can look at beta_0 with .intercept_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a493dc3c",
   "metadata": {},
   "source": [
    "Taken literally this says that an abalone of length 0 is 3.6 years old, which is somewhat nonsensical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd5b43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## We can look at beta_1 with .coef_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb48b78",
   "metadata": {},
   "source": [
    "This says that each one millimeter of length gives us an additional 0.0747 years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebcbf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plotting the model with our sample\n",
    "plt.figure(figsize=(8,5))\n",
    "\n",
    "plt.scatter(X['Length'], \n",
    "            y, \n",
    "            alpha=.7,\n",
    "            label=\"Sample\")\n",
    "\n",
    "plt.plot(np.linspace(10, 170, 100),\n",
    "         model.predict(\n",
    "            pd.DataFrame.from_dict(\n",
    "                {'Length': np.linspace(0, 160, 100)}\n",
    "                )\n",
    "            ),\n",
    "         'k',\n",
    "         label='Model $\\\\hat{f}$')\n",
    "\n",
    "plt.legend(fontsize=10)\n",
    "plt.xlabel(\"Length (mm)\", fontsize=12)\n",
    "plt.ylabel(\"Age (years)\", fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1e7835",
   "metadata": {},
   "source": [
    "#### WARNING\n",
    "\n",
    "We used this data to illustrate how to fit and interpret the results of a simple linear regression model.  We are leaving out a lot of important pieces of a good analysis!  \n",
    "\n",
    "To list just a few things we left out:\n",
    "\n",
    "1. We shouldn't even start building a model until we understand how it will be used.  Who is interested in the age of abalone and why?  Is our goal primarily predictive or inferential?  What margin of error on our estimates is acceptable?\n",
    "2. If our goal is predictive we should use cross validation to assess the model's generalization error.\n",
    "3. If our goal is inferential we might be interested in a confidence interval for the slope, or prediction intervals for new observations.  This is made more complicated by the (possible) presence of heteroscedasticity. See this post about [wandering schematic plots](https://stats.stackexchange.com/a/33033) for an idea of how to diagnose heteroscedasticity. \n",
    "4. We have only used one predictor in our model which does make it easy to use, but may leave a lot of \"signal\" on the table."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb119dc",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "\n",
    "This notebook was written for the Erd&#337;s Institute C&#337;de Data Science Boot Camp by Matthew Osborne, Ph. D., 2023.  Modified by Steven Gubkin 2024.\n",
    "\n",
    "Any potential redistributors must seek and receive permission from Matthew Tyler Osborne, Ph.D. prior to redistribution. Redistribution of the material contained in this repository is conditional on acknowledgement of Matthew Tyler Osborne, Ph.D.'s original authorship and sponsorship of the Erdős Institute as subject to the license (see License.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erdos_spring_2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
