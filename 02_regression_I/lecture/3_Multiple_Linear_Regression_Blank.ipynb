{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f1769a6",
   "metadata": {},
   "source": [
    "# Multiple Linear Regression\n",
    "\n",
    "## What we will accomplish\n",
    "\n",
    "In this notebook we will:\n",
    "- Introduce the multiple linear regression model,\n",
    "- Show how to fit the model using the <i>normal equation</i>,\n",
    "- Fit a model with `sklearn`.\n",
    "- Show how to use the normal equations to make a custom class which mimics the sklearn model object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f515b56",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from seaborn import set_style\n",
    "\n",
    "set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2838277",
   "metadata": {},
   "source": [
    "## The multiple linear regression model\n",
    "\n",
    "Suppose there is a quantitative variable you want to predict/model called $y$ and a set of $p$ features $x_1, x_2, \\dots x_p$, then the multiple linear regression model regressing $y$ on $x_1, \\dots, x_p$ is:\n",
    "\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_m x_p + \\epsilon = \\vec{x} \\cdot \\vec{\\beta} + \\epsilon,\n",
    "$$\n",
    "\n",
    "where $\\beta_0, \\dots, \\beta_p \\in \\mathbb{R}$ are constants, $\\vec{\\beta}$ is the $(p+1)$-vector of the $\\beta_i$ in numerical order, \n",
    "\n",
    "$$\n",
    "\\vec{x} = \\left(1, x_1, x_2, \\dots, x_p \\right)^\\top,\n",
    "$$\n",
    "\n",
    "and $\\epsilon \\sim N(0,\\sigma^2)$ is an error term independent of $\\vec{x}$.  Note that we have \"padded\" $\\vec{x}$ with an initial one to capture the constant term.\n",
    "\n",
    "### Fitting the model\n",
    "\n",
    "Suppose that we have $n$ observations $(\\vec{x}_i, y_i)$.   We can package these into an $n \\times (p+1)$ matrix $X$ and a $n$-vector $\\vec{y}$\n",
    "\n",
    "$$\n",
    "X = \\begin{bmatrix}\n",
    "1 & x_{11} & x_{12} & ... & x_{1p}\\\\\n",
    "1 & x_{21} & x_{22} & ... & x_{2p}\\\\\n",
    "  &        &        & \\vdots &    \\\\\n",
    "1 & x_{n1} & x_{n2} & ... & x_{np}\\\\  \n",
    "\\end{bmatrix}\n",
    "\n",
    "\\hphantom{fdsfds}\n",
    "\n",
    "\\vec{y} = \n",
    "\\begin{bmatrix}\n",
    "y_1\\\\y_2\\\\ \\vdots \\\\ y_n\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "In order to fit a multiple linear regression model regressing $y$ on $\\vec{x}$ using this data we return to the mean square error.\n",
    "\n",
    "$$\n",
    "\\operatorname{MSE}(\\vec{\\beta}) = \\frac{1}{n} \\sum_{i=1}^n \\left(y_i - f_{\\vec{\\beta}}(\\vec{x}_i)\\right)^2 = \\frac{1}{n} \\sum_{i=1}^n \\left(y_i - \\vec{x}_i^\\top \\vec{\\beta}\\right)^2,\n",
    "$$\n",
    "\n",
    "we can rewrite this using some linear algebra as:\n",
    "\n",
    "$$\n",
    "\\operatorname{MSE}(\\vec{\\beta})  = \\frac{1}{n} \\left\\vert \\vec{y} - X \\vec{\\beta}\\right\\vert^2\n",
    "$$\n",
    "\n",
    "We want to find the value of the parameter $\\vec{\\beta}$ which minimizes the MSE.\n",
    "\n",
    "As seen in Math Hour,  we can minimize it in two ways:\n",
    "\n",
    "* Using multivariable differential calculus:  find gradient of the MSE with respect of $\\beta$ and set it equal to zero.\n",
    "* Using linear algebra:  view this geometrically as projecting $\\vec{y}$ into the subspace spanned by the columns of $X$.\n",
    "\n",
    "Here is a short derivation using the linear algebra approach:  We are looking for $\\hat{\\beta}$ so that $\\vec{y} - X \\hat{\\beta}$ is orthogonal to $\\operatorname{Im}(X)$.  So \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "& X^\\top (\\vec{y} - X \\hat{\\beta}) = 0\\\\\n",
    "& X^\\top \\vec{y} - X^\\top X \\hat{\\beta} = 0 \\\\\n",
    "& X^\\top X \\hat{\\beta} = X^\\top \\vec{y}\\\\\\\n",
    "& \\hat{\\beta} = (X^\\top X)^{-1} X^\\top \\vec{y}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "This is the <i>ordinary least squares</i> estimate of the coefficient vector $\\hat{\\beta}$. Note that this formula is also sometimes called the <i>normal equation</i>.\n",
    "\n",
    "Note:  $X^\\top X$ is invertible if the columns of $X$ are linearly independent.  When this is not true we say that $X$ suffers from \"exact multicollinearity\".  Even if the columns of $X$ are linearly independent we can still have issues if they are \"close\" (i.e. if the condition number of $X$ is large)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76fa824",
   "metadata": {},
   "source": [
    "### A multiple linear regression model for abalone age\n",
    "\n",
    "We will demonstrate how to fit this model using `sklearn` with the abalone data.\n",
    "\n",
    "We will then write a naive custom class which functions similarly to the sklearn module to get a feel for what is going on \"under the hood\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcf800e",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "  \n",
    "# fetch dataset \n",
    "abalone = fetch_ucirepo(id=1) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = abalone.data.features \n",
    "y = abalone.data.targets \n",
    "\n",
    "continuous_features = ['Length', 'Diameter', 'Height', 'Whole_weight', 'Shucked_weight', 'Viscera_weight', 'Shell_weight']\n",
    "\n",
    "X.loc[:,continuous_features] *= 200\n",
    "\n",
    "y = y.Rings\n",
    "y += 1.5\n",
    "y.rename('Age', inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8823a31",
   "metadata": {},
   "source": [
    "### Using `sklearn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fac5f6",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "## import the LinearRegression object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15145ce7",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "## Make the model object\n",
    "\n",
    "model = \n",
    "\n",
    "## Fit the model object\n",
    "## note I do NOT have to use reshape here\n",
    "## because X_train is a 2D np.array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72be0d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## look at coef and intercept\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ac638d",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "## Make a prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e478ee19",
   "metadata": {},
   "source": [
    "### Implementing a custom linear regression class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c2b107",
   "metadata": {},
   "source": [
    "To give you a feeling for what is going on \"under the hood\" of an sklearn model object, we will write a minimal custom linear regression model object.  The actual sklearn implementation is much more sophisticated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265e170d",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "class CustomLinearRegression():\n",
    "    '''A minimal custom linear regression model object'''\n",
    "    def __init__(self):\n",
    "        self.beta = None\n",
    "    def fit(self,X,y):\n",
    "        '''Finds self.beta using the normal equation.  \n",
    "            X and y must be numpy arrays.\n",
    "            X must have shape (num_samples, num_features)\n",
    "            y must have shape (num_samples,)'''\n",
    "        X = np.hstack([np.ones((X.shape[0],1)), X]) # adds column of ones to X\n",
    "        XtX = np.dot(X.transpose(),X) \n",
    "        XtXinv = np.linalg.inv(XtX)\n",
    "        Xty = np.dot(X.transpose(),y)\n",
    "        self.beta = np.dot(XtXinv, Xty)\n",
    "    def predict(self,X):\n",
    "        X = np.hstack([np.ones((X.shape[0],1)), X]) # adds column of ones to X\n",
    "        return np.dot(X, self.beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917d5b4a",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "custom_model = CustomLinearRegression()\n",
    "custom_model.fit(X[continuous_features],y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db34b8f",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "custom_model.beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9bb01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.intercept_, model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44723611",
   "metadata": {},
   "source": [
    "#### Warning\n",
    "\n",
    "In the simple linear regression notebook we noted many caveats.  Those remarks apply equally well here!  If our goal is inference we have a new danger:  **multicollinearity** of the predictors!  Intuitively if some of our predictors are themselves strongly correlated (say length and diameter) then it becomes difficult to disambiguate their effects, leading to wider confidence intervals for the coefficients of these parameters.  Mathematically, multicollinearity means that $X^\\top X$ has a large condition number. As a result changes in $y$ can lead to large changes in $(X^\\top X)^{-1}(X^\\top y)$. We will return to these issues when we get to inference in lectures 5 and 6.\n",
    "\n",
    "Another major warning:  we didn't do any Exploratory Data Analysis (EDA) on our other predictors.  Some of these other predictors could, conceivably, have non-linear relationships with the outcome in which case applying \"vanilla\" linear regression would be inappropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7d1ea9",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "\n",
    "This notebook was written for the Erd&#337;s Institute C&#337;de Data Science Boot Camp by Matthew Osborne, Ph. D., 2023.  Modified by Steven Gubkin 2024.\n",
    "\n",
    "Any potential redistributors must seek and receive permission from Matthew Tyler Osborne, Ph.D. prior to redistribution. Redistribution of the material contained in this repository is conditional on acknowledgement of Matthew Tyler Osborne, Ph.D.'s original authorship and sponsorship of the Erdős Institute as subject to the license (see License.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erdos_spring_2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
