{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Session 6\n",
    "\n",
    "The problems in this notebook will cover the content covered in our Inference II lectures including:\n",
    "- Bootstapping\n",
    "- Model Specification Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.  Bootstrapping vs ANOVA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We introduce a new dataset which records the following variables for 200 students:\n",
    "\n",
    "* `read`: Score on a reading test\n",
    "* `math`: Score on a math test\n",
    "* `prog`: Categorical variable indicating program of study\n",
    "    * Takes values `vocational`, `general`, `academic`\n",
    "* `gre`:  The score on the GRE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"https://stats.idre.ucla.edu/stat/data/tobit.csv\", names = ['id','read', 'math', 'prog', 'gre'], header = 0, index_col= 'id')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (a) Exploratory Data Analysis\n",
    "\n",
    "Do a little EDA.  Some ideas include:\n",
    "\n",
    "* Comparing mean GRE score across different program types.\n",
    "* Plotting GRE against both \"read\" and \"math\" scores, perhaps colored using program type.\n",
    "\n",
    "Is there any other EDA you can think of?\n",
    "\n",
    "Did you notice anything interesting in your EDA?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare mean GRE score by program type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Make some plots \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (b) ANOVA\n",
    "\n",
    "The mean GRE score for each program type look different, but is this a statistically significant difference?\n",
    "\n",
    "As mentioned in the Linear Regression Inference lecture, one-way ANOVA is the same as an F-test comparing the two nested models\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\textrm{Reduced Model: }\\textrm{GRE} &= \\beta_0 + \\epsilon\\\\\n",
    "\\textrm{Full Model: }\\textrm{GRE} &= \\beta_0 + \\beta_1 \\textrm{Vocational} + \\beta_2 \\textrm{General} + \\epsilon\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The $p$-value of an $F$-test comparing the full model to the constant model is included in a `statsmodels model` summary.  So fit the full model to the data and see what you get!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# Hint:  C(prog) gives indicator variables for the levels of prog \n",
    "# with one omitted if a constant term is included in the model\n",
    "model = \n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note:  If you would like a traditional \"ANOVA table\" you can get one like this.abs\n",
    "# For statsmodels, ANOVA is just a special display \n",
    "# of a linear regression model with categorical variables\n",
    "\n",
    "anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "\n",
    "# The F statistic and p-value are the same as we obtained in the model summary above.\n",
    "print(anova_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (c) Bootstrap confidence intervals\n",
    "\n",
    "You should have obtained confidence intervals for the mean GRE score of each program type. \n",
    "\n",
    "We will now get confidence intervals by bootstrapping instead!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_ci(data, group_col, outcome_col, n_bootstrap=10000, ci=0.95):\n",
    "    '''\n",
    "    Inputs:\n",
    "        data:  A pandas dataframe.\n",
    "        group_col: The name of the column with the categorical variable.\n",
    "        outcome_col:  The name of the column with the continuous outcome.\n",
    "        n_bootstrap:  The number of bootstrap resamples.  We are resampling the rows of data.\n",
    "        ci: The nominal coverage of confidence interval.\n",
    "    \n",
    "    Outputs:\n",
    "        group_means:  A dictionary\n",
    "            keys:  One key for each unique level of group_col\n",
    "            values:  A tuple (original_mean, ci_lower, ci_upper)\n",
    "                original_mean: The group outcome mean in the original sample.\n",
    "                ci_lower:  Lower limit of the confidence interval.\n",
    "                ci_upper:  Upper limit of the confidence interval.\n",
    "    '''\n",
    "    group_means = {}\n",
    "    lower_percentile = (1 - ci) / 2\n",
    "    upper_percentile = 1 - lower_percentile\n",
    "    \n",
    "    # Get unique groups\n",
    "    groups = data[group_col].unique()\n",
    "    \n",
    "    # Perform percentile bootstrap for each group\n",
    "    for group in groups:\n",
    "        group_data = \n",
    "        original_mean =    # The original sample mean\n",
    "        \n",
    "        # Generate bootstrap samples and means\n",
    "        # Either loop over range(n_bootstraps) or write vectorized code\n",
    "        # Note that np.random.choice has a shape parameter which enables vectorization here!\n",
    "        # In either case you should now have a numpy array of bootstrap means.\n",
    "        # Call that array bootstrap_means\n",
    "\n",
    "\n",
    "\n",
    "        # Calculate percentile confidence intervals\n",
    "        ci_lower = \n",
    "        ci_upper = \n",
    "        \n",
    "        group_means[group] = (original_mean, ci_lower, ci_upper)\n",
    "    \n",
    "    return group_means\n",
    "\n",
    "bootstrap_results = bootstrap_ci(df, group_col='prog', outcome_col='gre')\n",
    "\n",
    "# Print the bootstrap mean and confidence intervals for each group\n",
    "for group, (mean, ci_lower, ci_upper) in bootstrap_results.items():\n",
    "    print(f\"Group: {group}, Mean: {mean:.2f}, 95% CI (Reverse Bootstrap): [{ci_lower:.2f}, {ci_upper:.2f}]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem we would like to regress `gre` on the other variables.  We *could* estimate this using vanilla OLS linear regression.  However the GRE has a maximum score of 800.  We can see that 17 of the students did actually achieve this score.  [This meme](https://www.youtube.com/shorts/Lb3lj4IhD0U) has a great point:  these students may have had an aptitude which was greater than what the test could measure.\n",
    "\n",
    "One tool for estimating the regression coefficients in this circumstance is [Tobit Regression](https://en.wikipedia.org/wiki/Tobit_model).  The idea is that we should model the GRE score as being given by a latent response which is linear in the predictors.  This latent response is then censored at the upper limit of 800.  We then estimate the model parameters using maximum likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately it doesn't seem that statsmodels (or any other Python library) contains an implementation.  So I implemented it from scratch by converting [Michael Clark's implementation in R](https://m-clark.github.io/models-by-example/tobit.html) to Python.\n",
    "\n",
    "Note:  This shows why understanding theory can be important!  Sometimes you really do need to \"roll your own\" model.\n",
    "\n",
    "You can check out the code for this TobitModel class which is found in  `tobit_model.py` in this folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is a custom model the API is a bit idiosyncratic, and it is definitely not optimized for either speed or usability.  However it does reproduce the results which Michael Clark was getting using R!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tobit_model import TobitModel\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how this works on some synthetic data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating some synthetic censored data\n",
    "X = np.linspace(0,10,100)\n",
    "y = 3 + X + np.random.randn(100)\n",
    "y = y * (y < 7) + 7 * (y > 7)\n",
    "\n",
    "# Fitting OLS linear regression model\n",
    "lr = LinearRegression()\n",
    "lr.fit(X.reshape(-1,1),y)\n",
    "\n",
    "# Fitting the Tobit model.  \n",
    "# Notice what parameters are needed for initialization.\n",
    "# ul is the (known) censoring upper limit.\n",
    "\n",
    "# TobitModel requires a design matrix with an initial column of ones.\n",
    "X_tb = np.ones((100,2))\n",
    "X_tb[:,1] = X\n",
    "\n",
    "tb = TobitModel(X = X_tb, y = y, ul = 7)\n",
    "tb.fit()\n",
    "\n",
    "plt.scatter(X, y)\n",
    "plt.plot(X, lr.predict(X.reshape(-1,1)), label = 'OLS fit')\n",
    "plt.plot(X, tb.predict(X_tb), label = 'Tobit fit')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [constant term, slope, log of variance]\n",
    "tb.params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the OLS model is inappropriate for censored data while the Tobit model does fine!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (a)\n",
    "\n",
    "Fit the Tobit model on the full dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make dummy variables for general and vocational levels of prog\n",
    "df['general'] = \n",
    "df['vocational'] =\n",
    "\n",
    "# Make a column of ones, needed for my implementation\n",
    "df['constant'] ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['constant','read', 'math', 'general', 'vocational']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the Tobit Model\n",
    "model = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{features[i]: model.params_[i] for i in range(len(features))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (b) Bootstrapping confidence intervals for the conditional target means.\n",
    "\n",
    "Since this model was not fit using ordinary least squares, the standard formula for the confidence intervals of conditional target means does not apply.  This is the kind of situation where bootstrapping really shines!\n",
    "\n",
    "Complete the definition of the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def bs_conditional_mean(X_train, y_train, ul, X_cond, num_bootstrap_samples):\n",
    "    '''\n",
    "    Finds the 'reverse' bootstrap confidence interval for the conditional outcome means of a Tobit model.\n",
    "\n",
    "    Inputs\n",
    "    X_train:  A numpy array with initial column of ones.  Shape is (nobs, 1 + number of features).\n",
    "    y_train:  A numpy array of shape (nobs,).\n",
    "    ul: Upper limit for Tobit Model.\n",
    "    X_cond:   Design matrix for the observations we want to condition on.\n",
    "    num_bootstrap_samples:  Self-explanatory\n",
    "\n",
    "    Outputs:\n",
    "    (lower_bound, upper_bound)\n",
    "    '''\n",
    "    y_cond = np.zeros((X_cond.shape[0], num_bootstrap_samples))\n",
    "    \n",
    "    for i in range(num_bootstrap_samples):\n",
    "        # sample the indices for bootstrap sampling\n",
    "        sample_indices = np.random.choice(range(X_train.shape[0]), size=X_train.shape[0], replace=True)\n",
    "        \n",
    "        # Slice X_boot using sampled indices\n",
    "        X_boot = \n",
    "        y_boot = \n",
    "\n",
    "        # Initialize and fit the Tobit model\n",
    "        model = \n",
    "        model.fit()\n",
    "\n",
    "        # Store predictions for the conditional mean\n",
    "        y_cond[:, i] = \n",
    "    \n",
    "    # Calculate the lower and upper bounds of the confidence intervals\n",
    "    lower_bound =\n",
    "    upper_bound = \n",
    "\n",
    "    return lower_bound, upper_bound\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this to find the confidence interval for the conditional mean GRE score for\n",
    "\n",
    "* `read = 60`, `math = 70`, `prog = general`\n",
    "* `read = 70`, `math = 40`, `prog = vocational`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint:  make a dataframe with the same column names as df[features].\n",
    "# You can make a dataframe from a dictionary.\n",
    "X_cond = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_ci, upper_ci = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confidence interval for mean GRE conditioned on `read = 60`, `math = 70`, `prog = general`\n",
    "lower_ci[0], upper_ci[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confidence interval for mean GRE conditioned on `read = 70`, `math = 40`, `prog = vocational`\n",
    "\n",
    "lower_ci[1], upper_ci[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erdos_fall_2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
